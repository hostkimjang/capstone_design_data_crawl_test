import asyncio
import json
import random
import time
from asyncio import wait_for
from time import sleep
import lxml
import platform
from bs4 import BeautifulSoup
import chardet
import pandas as pd
import pprint
import sqlite3
import zendriver as zd
import urllib
import re
import os
import sys

headless = False

def store_first_db():

    file_path = 'fulldata_07_24_04_P_ì¼ë°˜ìŒì‹ì .csv'

    with open(file_path, 'rb') as f:
        sample = f.read(1024 * 1024)  # 1MB

    # ìƒ˜í”Œ ë°ì´í„°ë¡œ ì¸ì½”ë”© ê°ì§€
    encoding_data = chardet.detect(sample)
    detected_encoding = encoding_data['encoding']
    print(f"Detected encoding: {detected_encoding}")

    data = pd.read_csv(file_path, encoding=detected_encoding, low_memory=False)
    pprint.pprint(data.head())
    print("Data loaded successfully.")

    # for column in data.columns:
    #     unique_values = data[column].dropna().unique()
    #     print(f"ğŸ“Œ ì»¬ëŸ¼: {column}")
    #     print(f"   â–¶ ê³ ìœ ê°’ {len(unique_values)}ê°œ")
    #     print(f"   â–¶ ìƒ˜í”Œ: {unique_values[:10]}")  # ì²˜ìŒ 10ê°œë§Œ ë³´ì—¬ì¤Œ
    #     print("-" * 50)

    #check data length
    print(f"Total records: {len(data)}")

    # íì—… ì œì™¸
    if 'ì˜ì—…ìƒíƒœëª…' in data.columns:
        print("ğŸ› ï¸ 'ì˜ì—…ìƒíƒœëª…' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ íì—… ë°ì´í„° ê±¸ëŸ¬ë‚´ëŠ” ì¤‘...")
        data = data[data['ì˜ì—…ìƒíƒœëª…'] != 'íì—…']
    else:
        print("âš ï¸ 'ì˜ì—…ìƒíƒœëª…' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. íì—… ë°ì´í„° ê±¸ëŸ¬ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    conn = sqlite3.connect('food_data.db')

    # DataFrameì„ SQLite í…Œì´ë¸”ë¡œ ì €ì¥
    data.to_sql('restaurants', conn, if_exists='replace', index=False)

    # ì—°ê²° ëŠê¸°
    conn.close()

    print(f"âœ… íì—… ì œì™¸ í›„ {len(data)}ê±´ ì €ì¥ ì™„ë£Œ! (DB: food_data.db, Table: restaurants)")

def load_10_restaurant_names_and_addresses():
    conn = sqlite3.connect('food_data.db')
    cursor = conn.cursor()

    cursor.execute("SELECT ì‚¬ì—…ì¥ëª…, ë„ë¡œëª…ì „ì²´ì£¼ì†Œ FROM restaurants LIMIT 200;")
    rows = cursor.fetchall()

    conn.close()

    restaurant_infos = []
    for row in rows:
        business_name, road_address = row
        if business_name and road_address:
            restaurant_infos.append((business_name, road_address))
    return restaurant_infos

def make_search_query(business_name, road_address):
    # ë„ë¡œëª… ì£¼ì†Œ ì• 3ë‹¨ê³„ê¹Œì§€ë§Œ
    parts = road_address.split()
    if len(parts) >= 3:
        filtered_address = ' '.join(parts[:3])
    else:
        filtered_address = road_address

    query = f"{business_name} {filtered_address}"
    # pprint.pprint(f"Search query: {query}")
    return query

def sanitize_filename(name):
    name = re.sub(r'[\\/*?:"<>|]', "", name).replace(" ", "_")
    return name[:100] if len(name) > 100 else name

async def load_page_with_wait(browser, url):
    page = await browser.get(url)

    tmp_parser = BeautifulSoup(await page.get_content(), "lxml")
    if any(err in tmp_parser.get_text().lower() for err in [
        "500", "internal server error", "proxy error", "nginx", "html error", "bad gateway"
    ]):
        time.sleep(3)
        raise Exception("âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: ì„œë²„ ì˜¤ë¥˜")
    await page.wait_for("div.place_business_list_wrapper", timeout=10)
    return page


async def load_page_with_wait02(browser, url):
    page = await browser.get(url)
    tmp_parser = BeautifulSoup(await page.get_content(), "lxml")
    if any(err in tmp_parser.get_text().lower() for err in [
        "500", "internal server error", "proxy error", "nginx", "html error", "bad gateway"
    ]):
        time.sleep(3)
        raise Exception("âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: ì„œë²„ ì˜¤ë¥˜")
    await page.wait_for("div.place_fixed_maintab", timeout=10)
    return page


async def with_retry(func, retries=5, delay=1):
    for attempt in range(retries):
        try:
            return await func()
        except Exception as e:
            print(f"âš ï¸ ì¬ì‹œë„ {attempt+1}/{retries} ì‹¤íŒ¨: {e}")
            await asyncio.sleep(delay)
    raise Exception("âŒ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")


async def wait_for_selector_with_retry(page, selector, timeout=10, interval=1):
    max_attempts = int(timeout / interval)
    for attempt in range(max_attempts):
        try:
            node = await page.select_all(selector)
            if node:
                return node
        except Exception:
            pass
        await asyncio.sleep(interval)
    raise TimeoutError(f"âŒ '{selector}' ë¡œë”© ì‹¤íŒ¨ (timeout={timeout}s)")


def extract_dynamic_place_info(soup: BeautifulSoup):
    place_info = {}
    title_block = soup.select("div.zD5Nm > div#_title")
    info_blocks = soup.select("div.PIbes > div.O8qbU")
    if title_block:
        title = title_block[0].get_text(strip=True)
        place_info["title"] = title

    for block in info_blocks:
        key_elem = block.select_one("strong > span.place_blind")
        value_block = block.select_one("div.vV_z_")
        if not key_elem or not value_block:
            continue
        key = key_elem.get_text(strip=True)
        if key == "ì£¼ì†Œ":
            addr = value_block.select_one("span.LDgIH")
            place_info["ì£¼ì†Œ"] = addr.text.strip() if addr else None
        elif key == "ì „í™”ë²ˆí˜¸":
            phone = value_block.select_one("span.xlx7Q")
            place_info["ì „í™”ë²ˆí˜¸"] = phone.text.strip() if phone else None
        elif key == "ì˜ì—…ì‹œê°„":
            status = value_block.select_one("em")
            hours = value_block.select_one("time")
            place_info["ì˜ì—…ìƒíƒœ"] = status.text.strip() if status else None
            place_info["ì˜ì—…ì‹œê°„"] = hours.text.strip() if hours else None
        elif key == "í™ˆí˜ì´ì§€":
            links = value_block.select("a.place_bluelink")
            place_info["í™ˆí˜ì´ì§€ë“¤"] = [a["href"] for a in links if a.get("href")]
        else:
            place_info[key] = value_block.get_text(strip=True)
    return place_info

def append_to_json_file(data, filepath):
    # íŒŒì¼ì´ ìˆìœ¼ë©´ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            try:
                existing = json.load(f)
            except json.JSONDecodeError:
                existing = []
    else:
        existing = []

    # ì¤‘ë³µ ë°©ì§€ (title ê¸°ì¤€)
    titles = {entry.get("title") for entry in existing}
    if data.get("title") not in titles:
        existing.append(data)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(existing, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“¦ ì €ì¥ ì™„ë£Œ: {data['title']}")
    else:
        print(f"âš ï¸ ì¤‘ë³µìœ¼ë¡œ ì €ì¥ ê±´ë„ˆëœ€: {data['title']}")


def log_error_json(error_info, filepath):
    error_info["timestamp"] = time.strftime("%Y-%m-%d %H:%M:%S")
    with open(filepath, "a", encoding="utf-8") as f:
        f.write(json.dumps(error_info, ensure_ascii=False) + "\n")
    print(f"âŒ ì˜¤ë¥˜ ê¸°ë¡ ì™„ë£Œ: {error_info['title'] if 'title' in error_info else 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}")


async def with_browser_retry(browser_ref, executable, browser_args, coro_fn, retries=5, delay=2):
    for attempt in range(retries):
        try:
            result = await coro_fn(browser_ref[0])
            html = await result.get_content()
            soup = BeautifulSoup(html, "lxml")
            page_text = soup.get_text().lower()
            for err in [
                "500", "internal server error", "proxy error", "nginx",
                "sigkill", "sigtrap", "aw snap", "í˜ì´ì§€ë¥¼ í‘œì‹œí•˜ëŠ” ë„ì¤‘ ë¬¸ì œ"
            ]:
                if err in page_text:
                    print(f"âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: ì—ëŸ¬ íƒì§€ë¨ â†’ '{err}'")
                    raise Exception(f"ğŸ›‘ HTML ë‚´ ì—ëŸ¬ í˜ì´ì§€ íƒì§€ë¨: '{err}'")
            return result
        except Exception as e:
            print(f"âš ï¸ ë¸Œë¼ìš°ì € ì‘ì—… ì‹¤íŒ¨ {attempt+1}/{retries}: {e}")
            try:
                await browser_ref[0].stop()
            except:
                pass
            print("ğŸ”„ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì¤‘...")
            browser_ref = [await start_browser(executable)]
            await asyncio.sleep(delay)
    raise Exception("âŒ ë¸Œë¼ìš°ì € ì¬ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨")


async def start_browser(executable):
    return await zd.start(
        headless=headless,
        browser_executable_path=executable,
        browser_args=browser_args
    )

async def wait_for_browser_ready(port, timeout=10):
    import socket
    for _ in range(timeout):
        try:
            with socket.create_connection(("127.0.0.1", port), timeout=1):
                return True
        except:
            await asyncio.sleep(1)
    raise Exception(f"ğŸš« ë¸Œë¼ìš°ì € í¬íŠ¸ {port} ì—°ê²° ì‹¤íŒ¨")


async def with_browser_get(url, browser_ref, executable, retries=5, delay=2):
    for attempt in range(retries):
        try:
            print(f"ğŸ“¡ ì‹œë„ {attempt+1}/{retries}: {url}")
            page = await browser_ref[0].get(url)
            html = await page.get_content()
            soup = BeautifulSoup(html, "lxml")
            if any(err in soup.get_text().lower() for err in [
                "500", "internal server error", "proxy error", "nginx", "html error", "bad gateway"
            ]):
                raise Exception("ğŸ›‘ HTML ë‚´ ì—ëŸ¬ í˜ì´ì§€ íƒì§€ë¨")
            return page
        except Exception as e:
            print(f"âš ï¸ ë¸Œë¼ìš°ì € ì‘ì—… ì‹¤íŒ¨ {attempt+1}/{retries}: {e}")
            print("ğŸ”„ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì¤‘...")
            try:
                await browser_ref[0].stop()
            except:
                pass
            browser_ref[0] = await start_browser(executable)
            await asyncio.sleep(delay)
    raise Exception("âŒ ë¸Œë¼ìš°ì € ì¬ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨")

async def crawler():
    restaurant_infos = load_10_restaurant_names_and_addresses()

    if not restaurant_infos:
        print("âŒ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ê²Œ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    print(f"â„¹ï¸ {len(restaurant_infos)}ê°œ ê°€ê²Œì— ëŒ€í•œ í¬ë¡¤ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    system = platform.platform()
    arch = platform.machine()
    executable = None

    print("ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸")
    print(f"ì‹œìŠ¤í…œ: {system}")
    print(f"ì•„í‚¤í…ì²˜: {arch}")

    if system != "mac" and arch in ("aarch64", "arm64"):
        print("ARM64 í™˜ê²½ ê°ì§€")
        if os.path.exists("/usr/bin/ungoogled-chromium"):
            executable = "/usr/bin/ungoogled-chromium"
        elif os.path.exists("/usr/bin/chromium"):
            executable = "/usr/bin/chromium"

    try:
        browser_ref = [await start_browser(executable)]

        print("âœ… Zendriver ì‹œì‘ ì™„ë£Œ.")
        success, fail, need_check = 0, 0, 0

        for index, (business_name, road_address) in enumerate(restaurant_infos):
            try:
                search_query = make_search_query(business_name, road_address)
                encoded_query = urllib.parse.quote(search_query)
                mob_url = f"https://m.place.naver.com/restaurant/list?query={encoded_query}&x=126&y=37"
                print(f"ğŸ”— [{index+1}] {search_query}")
                print(f"ğŸ”— [{index+1}] {search_query} URL: {mob_url}")

                page = await with_browser_get(mob_url, browser_ref, executable, retries=5, delay=3)
                await with_retry(lambda: page.wait_for("div.place_business_list_wrapper", timeout=10))
                soup = BeautifulSoup(await page.get_content(), "lxml")
                if soup.select("div[class='FYvSc']") or "ì¡°ê±´ì— ë§ëŠ” ì—…ì²´ê°€ ì—†ìŠµë‹ˆë‹¤" in soup.get_text():
                    print(f"âŒ [{index+1}] {search_query} ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    await page.save_screenshot(os.path.join(SCREENSHOT_DIR, f"no_store_{sanitize_filename(business_name)}.png"))
                    log_error_json({
                        "query": search_query,
                        "title": business_name,
                        "address": road_address,
                        "url": mob_url,
                        "type": "no_store",
                        "reason": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
                    }, os.path.join(ERROR_DIR, f"error_log_{start_index}.jsonl"))

                    need_check += 1
                    continue

                a_tags = soup.select("div.place_business_list_wrapper > ul > li a[href]")
                href_list = [a['href'] for a in a_tags]
                valid_links = list(set(
                    re.match(r"^/restaurant/\d+", href).group(0)
                    for href in href_list if re.match(r"^/restaurant/\d+", href)
                ))


                if not valid_links:
                    print(f"âš ï¸ [{index+1}] {search_query} ë§í¬ ì—†ìŒ")
                    need_check += 1
                    continue

                print(f"ğŸ”— [{index+1}] {search_query} {len(valid_links)}ê°œ ìœ íš¨ ë§í¬ ë°œê²¬")
                print(f"ğŸ”— [{index+1}] {search_query} {valid_links[:len(valid_links)]}")
                unique_links = list(set(valid_links))

                if len(unique_links) > 1:
                    print(f"âš ï¸ [{index+1}] {search_query} 1ì°¨ ê²€ìƒ‰ ìœ ì‚¬ë„ ë‹¤ì¤‘ ìƒì  ë°œê²¬: {unique_links}")
                    await page.save_screenshot(os.path.join(SCREENSHOT_DIR, f"multiple_stores_{sanitize_filename(business_name)}.png"))
                    log_error_json({
                        "query": search_query,
                        "title": business_name,
                        "address": road_address,
                        "url": mob_url,
                        "type": "multiple_stores",
                        "reason": "ìœ ì‚¬ë„ ë†’ì€ ìƒì ì´ 2ê°œ ì´ìƒ ì¡´ì¬",
                        "candidates": unique_links
                    }, os.path.join(ERROR_DIR, f"error_log_{start_index}.jsonl"))

                    need_check += 1
                    continue

                #await with_retry(lambda: page.get(f"https://m.place.naver.com{valid_links[0]}"))

                await with_browser_retry(
                    browser_ref, executable, browser_args,
                    lambda b: b.get(f"https://m.place.naver.com{valid_links[0]}")
                )
                print(f"ğŸ”— [{index+1}] {search_query} {valid_links[0]} ë¡œë”© ì™„ë£Œ")
                print(f"ğŸ”— [{index+1}] {search_query} 2ì°¨ URL: https://m.place.naver.com{valid_links[0]}")
                await with_retry(lambda: page.wait_for("div.place_fixed_maintab", timeout=10))
                #await wait_for_selector_with_retry(page, "div.place_fixed_maintab", timeout=15)

                parser = BeautifulSoup(await page.get_content(), "lxml")
                main_tab = parser.select_one('div[class="place_fixed_maintab"]')
                if main_tab:
                    href_list = [
                        a['href']
                        for a in main_tab.select('a[href]')
                        if a['href'].strip() and not a['href'].strip().startswith('#')
                    ]
                    print(f"ğŸ½ï¸ [{index+1}] {search_query} ìœ íš¨í•œ ë§í¬ ê°œìˆ˜: {len(href_list)}")
                    print(f"ğŸ½ï¸ [{index+1}] {search_query} ë§í¬: {href_list}")
                else:
                    print(f"âŒ [{index+1}] {search_query} place_fixed_maintab not found.")

                place_info = extract_dynamic_place_info(parser)

                data = {
                    "query": search_query,
                    "title": business_name,
                    "place_info": place_info,
                    "unique_links": unique_links,
                    "tab_list": href_list,
                    "url": mob_url
                }

                append_to_json_file(data, output_path)
                success += 1

                if (index + 1) % 10 == 0:
                    await page.close()
                    await browser_ref[0].stop()
                    print("ğŸ”„ ë©”ëª¨ë¦¬ ìœ ì¶œ ë°©ì§€ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì¤‘...")
                    browser_ref = [await start_browser(executable)]
                    print("âœ… ë©”ëª¨ë¦¬ ìœ ì¶œ ë°©ì§€ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì™„ë£Œ.")

            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")
                log_error_json({
                    "query": search_query,
                    "title": business_name,
                    "address": road_address,
                    "url": mob_url,
                    "type": "multiple_stores",
                    "reason": str(e),
                    "candidates": unique_links
                }, os.path.join(ERROR_DIR, f"error_log_{start_index}.jsonl"))
                fail += 1
                continue

        print(f"\nâœ… ì™„ë£Œ: {success} / âŒ ì‹¤íŒ¨: {fail} / âš ï¸ í™•ì¸ í•„ìš”: {need_check}")

    finally:
        await browser_ref[0].stop()
        print("ğŸ›‘ Zendriver ì¢…ë£Œ ì™„ë£Œ")


if __name__ == "__main__":
    #store_first_db()
    if not os.path.exists("screenshots"):
        os.makedirs("screenshots")

    if not os.path.exists("web_data"):
        os.makedirs("web_data")

    if not os.path.exists("error_logs"):
        os.makedirs("error_logs")

    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    SCREENSHOT_DIR = os.path.join(BASE_DIR, "screenshots")
    DATA_DIR = os.path.join(BASE_DIR, "web_data")
    ERROR_DIR = os.path.join(BASE_DIR, "error_logs")

    os.makedirs(SCREENSHOT_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(ERROR_DIR, exist_ok=True)

    print("ğŸ“‚ Screenshot ì €ì¥ ê²½ë¡œ:", SCREENSHOT_DIR)
    print("ğŸ“‚ WebData ì €ì¥ ê²½ë¡œ:", DATA_DIR)
    print("ğŸ“‚ ErrorLog ì €ì¥ ê²½ë¡œ:", ERROR_DIR)

    start_index = int(os.environ.get("START_INDEX", sys.argv[1] if len(sys.argv) > 1 else 0))
    output_path = os.path.join(DATA_DIR, f"output_{start_index}.json")

    browser_args = [
        "--no-sandbox",
        "--disable-dev-shm-usage",
        "--disable-setuid-sandbox",
        "--disable-software-rasterizer",
        "--disable-blink-features=AutomationControlled",
        "--window-size=1280x800",  # ë°˜ë“œì‹œ ì‚¬ì´ì¦ˆ ì§€ì •
        "--start-maximized",  # headlessì—ì„œë„ ìµœëŒ€í™”ì²˜ëŸ¼ ë³´ì´ê²Œ
        "--disable-infobars",
        "--disable-extensions",
        "--disable-popup-blocking",
        "--enable-logging=stderr",
        "--log-level=1",
    ]

    asyncio.run(crawler())