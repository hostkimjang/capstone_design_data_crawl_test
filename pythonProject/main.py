import asyncio
import random
import time
from time import sleep
import lxml
from bs4 import BeautifulSoup
import chardet
import pandas as pd
import pprint
import sqlite3
import zendriver as zd
import urllib
import re
import os

if not os.path.exists("screenshots"):
    os.makedirs("screenshots")

def store_first_db():

    file_path = 'fulldata_07_24_04_P_ì¼ë°˜ìŒì‹ì .csv'

    with open(file_path, 'rb') as f:
        sample = f.read(1024 * 1024)  # 1MB

    # ìƒ˜í”Œ ë°ì´í„°ë¡œ ì¸ì½”ë”© ê°ì§€
    encoding_data = chardet.detect(sample)
    detected_encoding = encoding_data['encoding']
    print(f"Detected encoding: {detected_encoding}")

    data = pd.read_csv(file_path, encoding=detected_encoding, low_memory=False)
    pprint.pprint(data.head())
    print("Data loaded successfully.")

    # for column in data.columns:
    #     unique_values = data[column].dropna().unique()
    #     print(f"ğŸ“Œ ì»¬ëŸ¼: {column}")
    #     print(f"   â–¶ ê³ ìœ ê°’ {len(unique_values)}ê°œ")
    #     print(f"   â–¶ ìƒ˜í”Œ: {unique_values[:10]}")  # ì²˜ìŒ 10ê°œë§Œ ë³´ì—¬ì¤Œ
    #     print("-" * 50)

    #check data length
    print(f"Total records: {len(data)}")

    # íì—… ì œì™¸
    if 'ì˜ì—…ìƒíƒœëª…' in data.columns:
        print("ğŸ› ï¸ 'ì˜ì—…ìƒíƒœëª…' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ íì—… ë°ì´í„° ê±¸ëŸ¬ë‚´ëŠ” ì¤‘...")
        data = data[data['ì˜ì—…ìƒíƒœëª…'] != 'íì—…']
    else:
        print("âš ï¸ 'ì˜ì—…ìƒíƒœëª…' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. íì—… ë°ì´í„° ê±¸ëŸ¬ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    conn = sqlite3.connect('food_data.db')

    # DataFrameì„ SQLite í…Œì´ë¸”ë¡œ ì €ì¥
    data.to_sql('restaurants', conn, if_exists='replace', index=False)

    # ì—°ê²° ëŠê¸°
    conn.close()

    print(f"âœ… íì—… ì œì™¸ í›„ {len(data)}ê±´ ì €ì¥ ì™„ë£Œ! (DB: food_data.db, Table: restaurants)")

def load_10_restaurant_names_and_addresses():
    conn = sqlite3.connect('food_data.db')
    cursor = conn.cursor()

    cursor.execute("SELECT ì‚¬ì—…ì¥ëª…, ë„ë¡œëª…ì „ì²´ì£¼ì†Œ FROM restaurants ;")
    rows = cursor.fetchall()

    conn.close()

    restaurant_infos = []
    for row in rows:
        business_name, road_address = row
        if business_name and road_address:
            restaurant_infos.append((business_name, road_address))
    return restaurant_infos

def make_search_query(business_name, road_address):
    # ë„ë¡œëª… ì£¼ì†Œ ì• 3ë‹¨ê³„ê¹Œì§€ë§Œ
    parts = road_address.split()
    if len(parts) >= 3:
        filtered_address = ' '.join(parts[:3])
    else:
        filtered_address = road_address

    query = f"{business_name} {filtered_address}"
    # pprint.pprint(f"Search query: {query}")
    return query

def sanitize_filename(name):
    # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±° (\ / * ? : " < > |)
    name = re.sub(r'[\\/*?:"<>|]', "", name)
    # ê³µë°±ì„ ë°‘ì¤„ë¡œ ë³€ê²½ (ì„ íƒ ì‚¬í•­)
    name = name.replace(" ", "_")
    # í•„ìš”ì‹œ ìµœëŒ€ ê¸¸ì´ ì œí•œ (ì„ íƒ ì‚¬í•­)
    max_len = 100
    if len(name) > max_len:
        name = name[:max_len]
    return name


async def crawler():
    restaurant_infos = load_10_restaurant_names_and_addresses()
    if not restaurant_infos:
        print("âŒ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ê²Œ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    print(f"â„¹ï¸ {len(restaurant_infos)}ê°œ ê°€ê²Œì— ëŒ€í•œ í¬ë¡¤ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    if not os.path.exists("screenshots"):
        os.makedirs("screenshots")

    try:
        print("ğŸš€ Zendriver ì‹œì‘ ì¤‘...")
        browser = await zd.start(headless=False)
        print("âœ… Zendriver ì‹œì‘ ì™„ë£Œ. ë¸Œë¼ìš°ì € ê°ì²´ í™•ë³´.")

        results = []

        for index, (business_name, road_address) in enumerate(restaurant_infos):
            search_query = make_search_query(business_name, road_address)
            encoded_query = urllib.parse.quote(search_query) # URL ì¸ì½”ë”©
            search_url = f"https://map.naver.com/p/search/{encoded_query}"

            print(f"\n--- {index+1}/{len(restaurant_infos)} ì²˜ë¦¬ ì¤‘: {search_query} ---")
            print(f"ğŸ”— URL: {search_url}")

            try:
                # 1. browser.get()ìœ¼ë¡œ ì´ë™í•˜ê³ , ë°˜í™˜ëœ page/tab ê°ì²´ë¥¼ ì‚¬ìš©
                page = await browser.get(search_url)
                print("ğŸŒ í˜ì´ì§€ ì´ë™ ì™„ë£Œ. ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° ì¤‘...") # browser.getì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦°ë‹¤ê³  ê°€ì •

                # --- ì´ì œ 'page' (Tab ê°ì²´)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì†Œ ì°¾ê¸° ---
                search_iframe_selector = "#searchIframe"
                entry_iframe_selector = "#entryIframe"

                pprint.pprint("ğŸ”„ searchIframe ë¡œë”© ëŒ€ê¸°ì¤‘...")
                await page.wait_for(search_iframe_selector, timeout=10000)
                pprint.pprint("âœ… searchIframe ë¡œë”© ì™„ë£Œ.")

                # tmp_content = await page.get_content()
                pprint.pprint('SearchIframe url í™•ì¸ì¤‘')
                iframe_elements_str_list = await page.select_all("#searchIframe")
                iframe_string = str(iframe_elements_str_list[0])
                match = re.search(r'src="([^"]*)"', iframe_string)
                pprint.pprint(f"iframe URL: {match.group(1)}")

                await page.get(match.group(1))
                tmp_content = await page.get_content()
                tmp_parse = BeautifulSoup(tmp_content, "lxml")

                if tmp_parse.select("div[class='FYvSc']"):
                    pprint.pprint("âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ.")
                    continue

                await page.get(search_url)
                pprint.pprint("ğŸ”„ entryIframe ë¡œë”© ëŒ€ê¸°ì¤‘...")
                await page.wait_for(search_iframe_selector, timeout=10000)
                await page.wait_for(entry_iframe_selector, timeout=10000)
                pprint.pprint("âœ… entryIframe ë¡œë”© ì™„ë£Œ.")
                iframe_elements_str_list = await page.select_all("#entryIframe")
                iframe_string = str(iframe_elements_str_list[0])
                match = re.search(r'src="([^"]*)"', iframe_string)
                pprint.pprint(f"iframe URL: {match.group(1)}")
                await page.get(match.group(1))
                time.sleep(1)


            except Exception as e:
                print(f"âŒ iframe ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

        # --- ëª¨ë“  ë£¨í”„ ì¢…ë£Œ í›„ ---
        print("\nğŸ‰ í¬ë¡¤ëŸ¬ ì‘ì—… ì™„ë£Œ.")

    except Exception as start_err:
        print(f"ğŸ’¥ Zendriver ì‹œì‘ ë˜ëŠ” ì „ì²´ í¬ë¡¤ë§ ê³¼ì •ì—ì„œ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {start_err}")

if __name__ == "__main__":
    #store_first_db()
    asyncio.run(crawler())