import asyncio
import json
import os
import random
import re
import sqlite3
import time
import requests
from datetime import datetime
from tqdm import tqdm
from playwright.async_api import async_playwright

DB_PATH = "/app/food_merged_final.db"  # Docker í™˜ê²½ì˜ DB ê²½ë¡œ
TABLE_NAME = "restaurant_merged"
BUSINESS_ID_COLUMN = "ë„¤ì´ë²„_PLACE_ID_URL"
MAX_PAGES = 10
MAX_RETRIES = 3
MIN_DELAY = 0.5  # ë¶„ì‚°ì²˜ë¦¬ë¥¼ ìœ„í•´ ë”œë ˆì´ ìµœì†Œí™”
MAX_DELAY = 2
GRAPHQL_URL = "https://api.place.naver.com/graphql"

# GraphQL ì¿¼ë¦¬ ë°ì´í„°
PHOTO_QUERY = """
query getPhotoViewerItems($input: PhotoViewerInput) {
  photoViewer(input: $input) {
    cursors {
      id
      startIndex
      hasNext
      lastCursor
      __typename
    }
    photos {
      viewId
      originalUrl
      originalDate
      width
      height
      title
      text
      desc
      link
      date
      photoType
      mediaType
      option {
        channelName
        dateString
        playCount
        likeCount
        __typename
      }
      to
      relation
      logId
      author {
        id
        nickname
        from
        imageUrl
        objectId
        url
        borderImageUrl
        __typename
      }
      votedKeywords {
        code
        iconUrl
        iconCode
        name
        __typename
      }
      visitCount
      originType
      isFollowing
      businessName
      rating
      externalLink {
        title
        url
        __typename
      }
      sourceTitle
      moment {
        channelId
        contentId
        momentId
        gdid
        blogRelation
        statAllowYn
        category
        docNo
        __typename
      }
      video {
        videoId
        videoUrl
        trailerUrl
        __typename
      }
      music {
        artists
        title
        __typename
      }
      clip {
        serviceType
        createdAt
        __typename
      }
      __typename
    }
    __typename
  }
}
"""

def log_failure(business_id, payload, error=None):
    with open("failed_requests.log", "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now()}] âŒ {business_id} ìš”ì²­ ì‹¤íŒ¨\n")
        if error:
            f.write(f"Error: {error}\n")
        f.write(f"Payload: {json.dumps(payload, ensure_ascii=False)}\n\n")

def make_payload(business_id, cursors):
    return {
        "operationName": "getPhotoViewerItems",
        "variables": {
            "input": {
                "businessId": business_id,
                "businessType": "restaurant",
                "cursors": cursors,
                "excludeAuthorIds": [],
                "excludeSection": [],
                "excludeClipIds": [],
                "dateRange": ""
            }
        },
        "query": PHOTO_QUERY
    }

def save_jsonl(filename, photos, output_path):
    filepath = os.path.join(output_path, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        for p in photos:
            f.write(json.dumps(p, ensure_ascii=False) + "\n")

def load_business_ids_range(start=0, end=100):
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute(f"""
            SELECT id, {BUSINESS_ID_COLUMN}
            FROM {TABLE_NAME}
            WHERE {BUSINESS_ID_COLUMN} IS NOT NULL
            LIMIT ? OFFSET ?
        """, (end - start, start))
        rows = cursor.fetchall()
        conn.close()
        ids = []
        for db_id, url in rows:
            match = re.search(r'/restaurant/(\d+)', url)
            if match:
                business_id = match.group(1)
                ids.append((db_id, business_id))
        return ids
    except Exception as e:
        print(f"âŒ DB ì ‘ê·¼ ì˜¤ë¥˜: {e}")
        return []

def random_delay():
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    time.sleep(delay)
    return delay

def make_request_with_cookies(business_id, cursors, browser_data, retry_count=0):
    """ë¸Œë¼ìš°ì €ì—ì„œ íšë“í•œ ì¿ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ requestsë¡œ ìš”ì²­"""
    try:
        payload = make_payload(business_id, cursors)
        
        # í—¤ë” ì„¤ì •
        headers = {
            'Content-Type': 'application/json',
            'User-Agent': browser_data['user_agent'],
            'Referer': f'https://m.place.naver.com/restaurant/{business_id}/photo',
            'Origin': 'https://m.place.naver.com',
            'Accept-Language': 'ko-KR,ko;q=0.9',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Sec-Ch-Ua': '"Chromium";v="136", "Google Chrome";v="136", "Not.A/Brand";v="99"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"macOS"',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-site'
        }
        
        # ìš”ì²­ ë³´ë‚´ê¸°
        response = requests.post(
            GRAPHQL_URL,
            json=payload,
            headers=headers,
            cookies=browser_data['cookies'],
            timeout=10
        )
        
        # ì‘ë‹µ í™•ì¸
        if response.status_code != 200:
            print(f"âš ï¸ API ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
            if retry_count < MAX_RETRIES:
                retry_delay = min(2 * (retry_count + 1), 10)  # ìµœëŒ€ 10ì´ˆê¹Œì§€ ëŒ€ê¸°
                print(f"ğŸ”„ {retry_delay:.1f}ì´ˆ í›„ ì¬ì‹œë„ ({retry_count + 1}/{MAX_RETRIES})...")
                time.sleep(retry_delay)
                return make_request_with_cookies(business_id, cursors, browser_data, retry_count + 1)
            return None
        
        data = response.json()
        return data
        
    except Exception as e:
        print(f"âŒ ìš”ì²­ ì˜ˆì™¸: {e}")
        if retry_count < MAX_RETRIES:
            retry_delay = min(2 * (retry_count + 1), 10)
            print(f"ğŸ”„ {retry_delay:.1f}ì´ˆ í›„ ì¬ì‹œë„ ({retry_count + 1}/{MAX_RETRIES})...")
            time.sleep(retry_delay)
            return make_request_with_cookies(business_id, cursors, browser_data, retry_count + 1)
        return None

async def extract_cookies_from_browser(page, business_id=None):
    """Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì¿ í‚¤ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ìœ íš¨í•œ URL ì„¤ì •
        url = f"https://m.place.naver.com/restaurant/{business_id}/home" if business_id else "https://m.place.naver.com/restaurant/list"
        
        # í˜ì´ì§€ ì´ë™
        await page.goto(url, wait_until="networkidle", timeout=30000)
        print(f"ğŸ”„ ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘... (URL: {url})")
        
        # í˜ì´ì§€ì™€ ìƒí˜¸ì‘ìš©í•˜ì—¬ ë” ë§ì€ ì¿ í‚¤ ìƒì„±
        await page.evaluate("""
            () => {
                window.scrollTo(0, 200);
                setTimeout(() => window.scrollTo(0, 400), 300);
                setTimeout(() => window.scrollTo(0, 100), 600);
            }
        """)
        await asyncio.sleep(1)
        
        # Playwrightì—ì„œ ì¿ í‚¤ ê°€ì ¸ì˜¤ê¸°
        cookies = await page.context.cookies()
        
        # ì¿ í‚¤ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        cookie_dict = {}
        for cookie in cookies:
            if '.naver.com' in cookie.get('domain', ''):
                cookie_dict[cookie['name']] = cookie['value']
        
        # ì¿ í‚¤ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if not cookie_dict:
            print("âš ï¸ ì¿ í‚¤ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¿ í‚¤ ì‚¬ìš©...")
            cookie_dict = {
                "NNB": "XLXNYI5U5MBTA",
                "PLACE_LANGUAGE": "ko"
            }
            
            # ê¸°ë³¸ ì¿ í‚¤ ì„¤ì •
            await page.context.add_cookies([
                {"name": "NNB", "value": "XLXNYI5U5MBTA", "domain": ".naver.com", "path": "/"},
                {"name": "PLACE_LANGUAGE", "value": "ko", "domain": ".place.naver.com", "path": "/"}
            ])
        
        # í˜„ì¬ user-agent ê°€ì ¸ì˜¤ê¸°
        user_agent = await page.evaluate("navigator.userAgent")
        
        return {
            'cookies': cookie_dict,
            'user_agent': user_agent
        }
    except Exception as e:
        print(f"âŒ ì¿ í‚¤ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì¿ í‚¤ ë°˜í™˜
        return {
            'cookies': {
                "NNB": "XLXNYI5U5MBTA",
                "PLACE_LANGUAGE": "ko"
            },
            'user_agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36"
        }

async def refresh_browser_cookies(page, business_id):
    """ë¸Œë¼ìš°ì € ì¿ í‚¤ë¥¼ ìƒˆë¡œê³ ì¹¨"""
    try:
        url = f"https://m.place.naver.com/restaurant/{business_id}/home"
        await page.goto(url, wait_until="networkidle", timeout=30000)
        print(f"ğŸ”„ ë¸Œë¼ìš°ì € ì„¸ì…˜ ê°±ì‹  ì¤‘... (URL: {url})")
        
        # ìƒí˜¸ì‘ìš©
        await page.evaluate("""
            () => {
                window.scrollTo(0, 200);
                setTimeout(() => window.scrollTo(0, 400), 300);
                setTimeout(() => window.scrollTo(0, 150), 600);
            }
        """)
        await asyncio.sleep(1)
        
        # ì¿ í‚¤ ê°€ì ¸ì˜¤ê¸°
        cookies = await page.context.cookies()
        
        # ì¿ í‚¤ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        cookie_dict = {}
        for cookie in cookies:
            if '.naver.com' in cookie.get('domain', ''):
                cookie_dict[cookie['name']] = cookie['value']
        
        # ì¿ í‚¤ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if not cookie_dict:
            print("âš ï¸ ê°±ì‹  ì¤‘ ì¿ í‚¤ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¿ í‚¤ ì‚¬ìš©...")
            cookie_dict = {
                "NNB": "XLXNYI5U5MBTA",
                "PLACE_LANGUAGE": "ko"
            }
        
        user_agent = await page.evaluate("navigator.userAgent")
        
        return {
            'cookies': cookie_dict,
            'user_agent': user_agent
        }
    except Exception as e:
        print(f"âŒ ì¿ í‚¤ ê°±ì‹  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì¿ í‚¤ ë°˜í™˜
        return {
            'cookies': {
                "NNB": "XLXNYI5U5MBTA",
                "PLACE_LANGUAGE": "ko"
            },
            'user_agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36"
        }

async def main():
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë²”ìœ„ ê°€ì ¸ì˜¤ê¸° (Docker í™˜ê²½)
    start = int(os.environ.get("START_INDEX", 0))
    end = int(os.environ.get("END_INDEX", 100))
    print(f"ğŸ“¦ í˜„ì¬ ì»¨í…Œì´ë„ˆëŠ” {start} ~ {end} ë²”ìœ„ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.")

    # ë¹„ì¦ˆë‹ˆìŠ¤ ID ë¡œë”©
    business_ids = load_business_ids_range(start, end)
    if not business_ids:
        print("âš ï¸ ê°€ì ¸ì˜¬ ì—…ì²´ IDê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    total = len(business_ids)
    print(f"ğŸ”¢ ì´ {total}ê°œ ì—…ì²´ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    output_path = os.path.join(BASE_DIR, "crawl_photo")
    os.makedirs(output_path, exist_ok=True)

    # ê²°ê³¼ ì¹´ìš´í„° ì´ˆê¸°í™”
    success_count = 0
    skip_count = 0
    error_count = 0
    cookie_refresh_count = 0

    # Playwright ì´ˆê¸°í™” - Dockerì—ì„œ ì‹¤í–‰ ì‹œ headless ëª¨ë“œ í•„ìˆ˜
    print("ğŸŒ Playwright ì´ˆê¸°í™” ì¤‘...")
    async with async_playwright() as p:
        # ë¸Œë¼ìš°ì € ì‹œì‘ (Docker í™˜ê²½ì—ì„œëŠ” headless=True í•„ìˆ˜)
        browser = await p.chromium.launch(headless=True)
        
        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = await browser.new_context(
            viewport={"width": 1280, "height": 800},
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36"
        )
        
        # ì²« í˜ì´ì§€ ìƒì„±
        page = await context.new_page()
        
        # ì²« ë²ˆì§¸ ì—…ì²´ ì •ë³´ë¡œ ì´ˆê¸° ì¿ í‚¤ íšë“
        if business_ids:
            first_db_id, first_business_id = business_ids[0]
            print(f"ğŸ”‘ ì²« ë²ˆì§¸ ì—…ì²´({first_business_id})ë¡œ ì´ˆê¸° ì¿ í‚¤ íšë“ ì¤‘...")
            browser_data = await extract_cookies_from_browser(page, first_business_id)
            print(f"ğŸ“ ì¿ í‚¤ íšë“ ì™„ë£Œ: {len(browser_data['cookies'])} ê°œì˜ ì¿ í‚¤")
        else:
            print("âš ï¸ ì—…ì²´ IDê°€ ì—†ì–´ ê¸°ë³¸ ì¿ í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            browser_data = {
                'cookies': {
                    "NNB": "XLXNYI5U5MBTA",
                    "PLACE_LANGUAGE": "ko"
                },
                'user_agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36"
            }

        # ê° ì—…ì²´ì— ëŒ€í•´ í¬ë¡¤ë§ ìˆ˜í–‰
        for i, (db_id, business_id) in enumerate(tqdm(business_ids, desc="ğŸ“¦ ë²”ìœ„ ë‚´ ì—…ì²´ ì²˜ë¦¬")):
            try:
                print(f"\nğŸš€ í¬ë¡¤ë§ ì‹œì‘: {business_id} (DB ID={db_id})")
                
                # ì£¼ê¸°ì ìœ¼ë¡œ ì¿ í‚¤ ë¦¬í”„ë ˆì‹œ (10ê°œ ì—…ì²´ë§ˆë‹¤)
                if i > 0 and i % 10 == 0:
                    print("ğŸ”„ ë¸Œë¼ìš°ì € ì¿ í‚¤ ë¦¬í”„ë ˆì‹œ ì¤‘...")
                    browser_data = await refresh_browser_cookies(page, business_id)
                    cookie_refresh_count += 1
                    print(f"ğŸ“ ì¿ í‚¤ ë¦¬í”„ë ˆì‹œ ì™„ë£Œ (#{cookie_refresh_count})")
                
                # ëª¨ë“  ì‚¬ì§„ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
                all_photos = []
                
                # ì‹œì‘ ì»¤ì„œ ì„¤ì •
                cursors = [
                    {"id": "biz"},
                    {"id": "cp0"},
                    {"id": "visitorReview"},
                    {"id": "clip"},
                    {"id": "imgSas"}
                ]

                # í˜ì´ì§€ë³„ë¡œ ì‚¬ì§„ ê°€ì ¸ì˜¤ê¸°
                for page_num in range(MAX_PAGES):
                    print(f"ğŸ“„ í˜ì´ì§€ {page_num + 1} ìš”ì²­ ì¤‘...")
                    
                    # requestsë¡œ ìš”ì²­ (ë¸Œë¼ìš°ì € ì¿ í‚¤ ì‚¬ìš©)
                    data = make_request_with_cookies(business_id, cursors, browser_data)
                    
                    # ìš”ì²­ ì‹¤íŒ¨ ì‹œ ì¿ í‚¤ ê°±ì‹  í›„ ì¬ì‹œë„
                    if not data:
                        print(f"âš ï¸ ìš”ì²­ ì‹¤íŒ¨ â†’ ë¸Œë¼ìš°ì € ì¿ í‚¤ ê°±ì‹  í›„ ì¬ì‹œë„")
                        browser_data = await refresh_browser_cookies(page, business_id)
                        cookie_refresh_count += 1
                        
                        # ì¿ í‚¤ ê°±ì‹  í›„ ì¬ì‹œë„
                        data = make_request_with_cookies(business_id, cursors, browser_data)
                        if not data:
                            print(f"âš ï¸ ì¬ì‹œë„ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
                            skip_count += 1
                            break

                    # ì‘ë‹µì—ì„œ ì‚¬ì§„ ì •ë³´ ì¶”ì¶œ
                    viewer = data.get('data', {}).get('photoViewer', {})
                    photos = viewer.get('photos') or []
                    cursors_data = viewer.get('cursors', [])

                    # ì‚¬ì§„ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
                    if not photos:
                        print(f"âš ï¸ ì‚¬ì§„ ì—†ìŒ, ì¢…ë£Œ")
                        break

                    # ê° ì‚¬ì§„ ì •ë³´ ì €ì¥
                    for photo in photos:
                        author = photo.get("author") or {}
                        all_photos.append({
                            "url": photo.get("originalUrl"),
                            "desc": photo.get("desc"),
                            "author": author.get("nickname"),
                            "video": photo.get("video"),
                            "width": photo.get("width"),
                            "height": photo.get("height"),
                            "date": photo.get("date"),
                            "viewId": photo.get("viewId")
                        })

                    # ë‹¤ìŒ í˜ì´ì§€ ì»¤ì„œ í™•ì¸
                    next_cursor = None
                    for cursor in cursors_data:
                        if cursor["id"] == "biz" and cursor.get("hasNext") and cursor.get("lastCursor"):
                            next_cursor = cursor["lastCursor"]
                    
                    # ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                    if not next_cursor:
                        print(f"âœ… ë‹¤ìŒ ì»¤ì„œ ì—†ìŒ, ì¢…ë£Œ")
                        break
                        
                    # ë‹¤ìŒ í˜ì´ì§€ ì»¤ì„œ ì„¤ì •
                    cursors[0] = {"id": "biz", "lastCursor": next_cursor}
                    
                    # í˜ì´ì§€ ê°„ ë”œë ˆì´
                    delay = random_delay()
                    print(f"â±ï¸ ë‹¤ìŒ í˜ì´ì§€ ìš”ì²­ê¹Œì§€ {delay:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")

                # ì‚¬ì§„ ì €ì¥
                if all_photos:
                    filename = f"{db_id}_photo_{business_id}.jsonl"
                    save_jsonl(filename, all_photos, output_path)
                    print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename} ({len(all_photos)}ì¥)")
                    success_count += 1
                else:
                    print(f"âš ï¸ ìˆ˜ì§‘ëœ ì‚¬ì§„ ì—†ìŒ")
                    skip_count += 1
                    
                # ì—…ì²´ ê°„ ë”œë ˆì´
                if db_id != business_ids[-1][0]:  # ë§ˆì§€ë§‰ ì—…ì²´ê°€ ì•„ë‹ˆë©´
                    delay = random.uniform(1, 3)
                    print(f"â±ï¸ ë‹¤ìŒ ì—…ì²´ í¬ë¡¤ë§ê¹Œì§€ {delay:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(delay)

            except Exception as e:
                print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
                log_failure(business_id, {}, error=str(e))
                error_count += 1
                # ì—ëŸ¬ í›„ ë³µêµ¬ ì‹œê°„
                time.sleep(random.uniform(3, 5))
                continue

        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        await browser.close()

    # ê²°ê³¼ ìš”ì•½
    print("\nğŸ“Š í¬ë¡¤ë§ ìš”ì•½")
    print(f"ğŸ”¢ ì „ì²´ ì—…ì²´ ìˆ˜: {total}")
    print(f"âœ… ì„±ê³µ: {success_count}")
    print(f"âš ï¸ ìŠ¤í‚µ: {skip_count}")
    print(f"âŒ ì‹¤íŒ¨: {error_count}")
    print(f"ğŸ”„ ì¿ í‚¤ ë¦¬í”„ë ˆì‹œ: {cookie_refresh_count}íšŒ")

if __name__ == "__main__":
    asyncio.run(main())
