import pprint
import sqlite3
import json
import time
import os


def connect_merged_db() -> sqlite3.Connection:
    """
    Connect to the merged database and attach the original food_data.db for read access.
    """
    conn = sqlite3.connect('food_merged.db')
    conn.execute("ATTACH DATABASE 'food_data.db' AS orig;")  # ì›ë³¸ DB attach
    return conn

def crawl_data_json_load(datapath):
    with open(datapath, 'r', encoding='utf-8') as f:
        return json.load(f)

def error_data_json_load(error_data_path):
    with open(error_data_path, 'r', encoding='utf-8') as f:
        return [json.loads(line.strip()) for line in f if line.strip()]

def create_merged_table(cursor):
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS restaurant_merged (
        ì‚¬ì—…ì¥ëª… TEXT,
        ì¸í—ˆê°€ì¼ì TEXT,
        ì˜ì—…ìƒíƒœëª… TEXT,
        ìƒì„¸ì˜ì—…ìƒíƒœëª… TEXT,
        ì†Œì¬ì§€ì „ì²´ì£¼ì†Œ TEXT,
        ë„ë¡œëª…ì „ì²´ì£¼ì†Œ TEXT,
        ë„ë¡œëª…ìš°í¸ë²ˆí˜¸ REAL,
        ìµœì¢…ìˆ˜ì •ì‹œì  TEXT,
        ë°ì´í„°ê°±ì‹ ì¼ì TEXT,
        ì—…íƒœêµ¬ë¶„ëª… TEXT,
        ë„¤ì´ë²„_ìƒí˜¸ëª… TEXT,
        ë„¤ì´ë²„_ì£¼ì†Œ TEXT,
        ë„¤ì´ë²„_ì „í™”ë²ˆí˜¸ TEXT,
        ë„¤ì´ë²„_URL TEXT,
        ë„¤ì´ë²„_PLACE_ID_URL TEXT,
        ë„¤ì´ë²„_place_info TEXT,
        ë„¤ì´ë²„_tab_list TEXT
    );
    """)


def crawl_data_process(datapath, error_data_path):
    conn = connect_merged_db()
    cursor = conn.cursor()
    create_merged_table(cursor)

    data = crawl_data_json_load(datapath)

    for idx, item in enumerate(data, 1):
        title = item.get("title", "").strip()
        query = item.get("query", "").strip()
        road_address = query[len(title):].strip()

        sql = f"""
        SELECT ì‚¬ì—…ì¥ëª…, ì¸í—ˆê°€ì¼ì, ì˜ì—…ìƒíƒœëª…, ìƒì„¸ì˜ì—…ìƒíƒœëª…,
               ì†Œì¬ì§€ì „ì²´ì£¼ì†Œ, ë„ë¡œëª…ì „ì²´ì£¼ì†Œ, ë„ë¡œëª…ìš°í¸ë²ˆí˜¸,
               ìµœì¢…ìˆ˜ì •ì‹œì , ë°ì´í„°ê°±ì‹ ì¼ì, ì—…íƒœêµ¬ë¶„ëª…
        FROM orig.restaurants  -- âœ… ì›ë³¸ DBì—ì„œ ì¡°íšŒ
        WHERE ì‚¬ì—…ì¥ëª… LIKE '{title}%'
          AND ë„ë¡œëª…ì „ì²´ì£¼ì†Œ LIKE '{road_address}%'
        LIMIT 1;
        """

        cursor.execute(sql)
        result = cursor.fetchone()
        if not result:
            print(f"[{idx}] âŒ No match for {title}")
            continue

        update_sql = f"""
        UPDATE orig.restaurants
        SET CRAWL = 1
        WHERE ì‚¬ì—…ì¥ëª… LIKE ? AND ë„ë¡œëª…ì „ì²´ì£¼ì†Œ LIKE ?
        """
        cursor.execute(update_sql, (f'{title}%', f'{road_address}%'))

        # ë„¤ì´ë²„ í¬ë¡¤ë§ ë°ì´í„°
        place = item.get("place_info", {})
        naver_name = place.get("title", "")
        naver_addr = place.get("ì£¼ì†Œ", "")
        naver_tel = place.get("ì „í™”ë²ˆí˜¸", "")
        naver_url = item.get("url", "")
        unique_links  = item.get("unique_links", [])
        place_info_json = json.dumps(item.get("place_info", {}), ensure_ascii=False)
        tab_list_json = json.dumps(item.get("tab_list", []), ensure_ascii=False)
        naver_place_id_url = unique_links[0] if unique_links else ""

        cursor.execute("""
        INSERT INTO restaurant_merged (
            ì‚¬ì—…ì¥ëª…, ì¸í—ˆê°€ì¼ì, ì˜ì—…ìƒíƒœëª…, ìƒì„¸ì˜ì—…ìƒíƒœëª…,
            ì†Œì¬ì§€ì „ì²´ì£¼ì†Œ, ë„ë¡œëª…ì „ì²´ì£¼ì†Œ, ë„ë¡œëª…ìš°í¸ë²ˆí˜¸,
            ìµœì¢…ìˆ˜ì •ì‹œì , ë°ì´í„°ê°±ì‹ ì¼ì, ì—…íƒœêµ¬ë¶„ëª…,
            ë„¤ì´ë²„_ìƒí˜¸ëª…, ë„¤ì´ë²„_ì£¼ì†Œ, ë„¤ì´ë²„_ì „í™”ë²ˆí˜¸,
            ë„¤ì´ë²„_URL, ë„¤ì´ë²„_PLACE_ID_URL,
            ë„¤ì´ë²„_place_info, ë„¤ì´ë²„_tab_list
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, result + (
            naver_name, naver_addr, naver_tel,
            naver_url, naver_place_id_url,
            place_info_json, tab_list_json
        ))

    conn.commit()
    conn.close()
    print("âœ… ë³‘í•© ë° ì €ì¥ ì™„ë£Œ")

def process_error_data(error_data_path):
    data = error_data_json_load(error_data_path)
    conn = connect_merged_db()
    cursor = conn.cursor()

    updated_count = 0
    for item in data:
        title = item.get("title", "").strip()
        query = item.get("query", "").strip()
        road_address = query[len(title):].strip()
        reason = item.get("reason", "Unknown")

        update_sql = """
        UPDATE orig.restaurants
        SET crawl_error = 1, error_reason = ?
        WHERE ì‚¬ì—…ì¥ëª… LIKE ? AND ë„ë¡œëª…ì „ì²´ì£¼ì†Œ LIKE ?
        """
        cursor.execute(update_sql, (reason, f'{title}%', f'{road_address}%'))
        updated_count += cursor.rowcount

    conn.commit()
    conn.close()
    print(f"âœ… ì˜¤ë¥˜ í•­ëª© {updated_count}ê±´ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")


if __name__ == "__main__":
    # web_data ë””ë ‰í† ë¦¬ ë‚´ë¶€ ëª¨ë“  .json íŒŒì¼
    web_data_dir = 'web_data'
    web_files = sorted([
        os.path.join(web_data_dir, f)
        for f in os.listdir(web_data_dir)
        if f.endswith('.json')
    ])

    # error_logs ë””ë ‰í† ë¦¬ ë‚´ë¶€ ëª¨ë“  .jsonl íŒŒì¼
    error_log_dir = 'error_logs'
    error_files = sorted([
        os.path.join(error_log_dir, f)
        for f in os.listdir(error_log_dir)
        if f.endswith('.jsonl')
    ])

    for datapath, error_data_path in zip(web_files, error_files):
        print(f"\nğŸ“ ì²˜ë¦¬ ì¤‘: {datapath} / {error_data_path}")
        crawl_data_process(datapath, error_data_path)
        process_error_data(error_data_path)