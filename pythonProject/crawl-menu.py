import asyncio
import json
import random
import time
import lxml
import platform
from bs4 import BeautifulSoup
import chardet
import pandas as pd
import pprint
import sqlite3
import zendriver as zd
import urllib
import re
from re import search, sub, compile as re_compile # compile ì¶”ê°€
import os
import sys

headless = False

def load_10_restaurant_names_and_addresses():
    conn = sqlite3.connect('food_merged_final.db')
    cursor = conn.cursor()
    cursor.execute("SELECT ID, ì‚¬ì—…ì¥ëª…, ë„¤ì´ë²„_PLACE_ID_URL FROM restaurant_merged  LIMIT 200;")
    rows = cursor.fetchall()

    conn.close()

    restaurant_infos = []
    for row in rows:
        id, business_name, road_address = row
        if id and business_name and road_address:
            restaurant_infos.append((id, business_name, road_address))
    return restaurant_infos

def load_restaurant_subset(start, end):
    conn = sqlite3.connect("/app/food_merged_final.db")
    cursor = conn.cursor()
    cursor.execute(
        "SELECT ID, ì‚¬ì—…ì¥ëª…, ë„¤ì´ë²„_PLACE_ID_URL FROM restaurant_merged LIMIT ? OFFSET ?",
        (end - start, start),
    )
    rows = cursor.fetchall()
    conn.close()
    return [(id, name, naver_id) for id, name, naver_id in rows if name and id]

def make_search_query(business_name, road_address):
    # ë„ë¡œëª… ì£¼ì†Œ ì• 3ë‹¨ê³„ê¹Œì§€ë§Œ
    parts = road_address.split()
    if len(parts) >= 3:
        filtered_address = ' '.join(parts[:3])
    else:
        filtered_address = road_address

    query = f"{business_name} {filtered_address}"
    # pprint.pprint(f"Search query: {query}")
    return query

def sanitize_filename(name):
    name = re.sub(r'[\\/*?:"<>|]', "", name).replace(" ", "_")
    return name[:100] if len(name) > 100 else name

async def load_page_with_wait(browser, url):
    page = await browser.get(url)

    tmp_parser = BeautifulSoup(await page.get_content(), "lxml")
    if any(err in tmp_parser.get_text().lower() for err in [
        "500", "internal server error", "proxy error", "nginx", "html error", "bad gateway"
    ]):
        time.sleep(3)
        raise Exception("âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: ì„œë²„ ì˜¤ë¥˜")
    await page.wait_for("div.place_business_list_wrapper", timeout=10)
    return page


async def load_page_with_wait02(browser, url):
    page = await browser.get(url)
    tmp_parser = BeautifulSoup(await page.get_content(), "lxml")
    if any(err in tmp_parser.get_text().lower() for err in [
        "500", "internal server error", "proxy error", "nginx", "html error", "bad gateway"
    ]):
        time.sleep(3)
        raise Exception("âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: ì„œë²„ ì˜¤ë¥˜")
    await page.wait_for("div.place_fixed_maintab", timeout=10)
    return page


async def with_retry(func, retries=5, delay=1):
    for attempt in range(retries):
        try:
            return await func()
        except Exception as e:
            print(f"âš ï¸ ì¬ì‹œë„ {attempt+1}/{retries} ì‹¤íŒ¨: {e}")
            await asyncio.sleep(delay)
    raise Exception("âŒ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")


async def wait_for_selector_with_retry(page, selector, timeout=10, interval=1):
    max_attempts = int(timeout / interval)
    for attempt in range(max_attempts):
        try:
            node = await page.select_all(selector)
            if node:
                return node
        except Exception:
            pass
        await asyncio.sleep(interval)
    raise TimeoutError(f"âŒ '{selector}' ë¡œë”© ì‹¤íŒ¨ (timeout={timeout}s)")


def extract_dynamic_place_info(soup: BeautifulSoup):
    place_info = {}
    title_block = soup.select("div.zD5Nm > div#_title")
    info_blocks = soup.select("div.PIbes > div.O8qbU")
    if title_block:
        title = title_block[0].get_text(strip=True)
        place_info["title"] = title

    for block in info_blocks:
        key_elem = block.select_one("strong > span.place_blind")
        value_block = block.select_one("div.vV_z_")
        if not key_elem or not value_block:
            continue
        key = key_elem.get_text(strip=True)
        if key == "ì£¼ì†Œ":
            addr = value_block.select_one("span.LDgIH")
            place_info["ì£¼ì†Œ"] = addr.text.strip() if addr else None
        elif key == "ì „í™”ë²ˆí˜¸":
            phone = value_block.select_one("span.xlx7Q")
            place_info["ì „í™”ë²ˆí˜¸"] = phone.text.strip() if phone else None
        elif key == "ì˜ì—…ì‹œê°„":
            status = value_block.select_one("em")
            hours = value_block.select_one("time")
            place_info["ì˜ì—…ìƒíƒœ"] = status.text.strip() if status else None
            place_info["ì˜ì—…ì‹œê°„"] = hours.text.strip() if hours else None
        elif key == "í™ˆí˜ì´ì§€":
            links = value_block.select("a.place_bluelink")
            place_info["í™ˆí˜ì´ì§€ë“¤"] = [a["href"] for a in links if a.get("href")]
        else:
            place_info[key] = value_block.get_text(strip=True)
    return place_info

def append_to_json_file(data, filepath):
    # íŒŒì¼ì´ ìˆìœ¼ë©´ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            try:
                existing = json.load(f)
            except json.JSONDecodeError:
                existing = []
    else:
        existing = []

    # ì¤‘ë³µ ë°©ì§€ (title ê¸°ì¤€)
    titles = {entry.get("title") for entry in existing}
    if data.get("title") not in titles:
        existing.append(data)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(existing, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“¦ ì €ì¥ ì™„ë£Œ: {data['title']}")
    else:
        print(f"âš ï¸ ì¤‘ë³µìœ¼ë¡œ ì €ì¥ ê±´ë„ˆëœ€: {data['title']}")


def log_error_json(error_info, filepath):
    error_info["timestamp"] = time.strftime("%Y-%m-%d %H:%M:%S")
    with open(filepath, "a", encoding="utf-8") as f:
        f.write(json.dumps(error_info, ensure_ascii=False) + "\n")
    print(f"âŒ ì˜¤ë¥˜ ê¸°ë¡ ì™„ë£Œ: {error_info['title'] if 'title' in error_info else 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}")


async def with_browser_retry(browser_ref, executable, browser_args, coro_fn, retries=5, delay=2):
    for attempt in range(retries):
        try:
            result = await coro_fn(browser_ref[0])
            html = await result.get_content()
            soup = BeautifulSoup(html, "lxml")
            page_text = soup.get_text().lower()
            for err in [
                "500", "internal server error", "proxy error", "nginx",
                "sigkill", "sigtrap", "aw snap", "í˜ì´ì§€ë¥¼ í‘œì‹œí•˜ëŠ” ë„ì¤‘ ë¬¸ì œ"
            ]:
                if err in page_text:
                    print(f"âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: ì—ëŸ¬ íƒì§€ë¨ â†’ '{err}'")
                    raise Exception(f"ğŸ›‘ HTML ë‚´ ì—ëŸ¬ í˜ì´ì§€ íƒì§€ë¨: '{err}'")
            return result
        except Exception as e:
            print(f"âš ï¸ ë¸Œë¼ìš°ì € ì‘ì—… ì‹¤íŒ¨ {attempt+1}/{retries}: {e}")
            try:
                await browser_ref[0].stop()
            except:
                pass
            print("ğŸ”„ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì¤‘...")
            browser_ref = [await start_browser(executable)]
            await asyncio.sleep(delay)
    raise Exception("âŒ ë¸Œë¼ìš°ì € ì¬ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨")


async def start_browser(executable):
    return await zd.start(
        headless=headless,
        browser_executable_path=executable,
        browser_args=browser_args
    )

async def wait_for_browser_ready(port, timeout=10):
    import socket
    for _ in range(timeout):
        try:
            with socket.create_connection(("127.0.0.1", port), timeout=1):
                return True
        except:
            await asyncio.sleep(1)
    raise Exception(f"ğŸš« ë¸Œë¼ìš°ì € í¬íŠ¸ {port} ì—°ê²° ì‹¤íŒ¨")


async def with_browser_get(url, browser_ref, executable, retries=5, delay=2):
    for attempt in range(retries):
        try:
            print(f"ğŸ“¡ ì‹œë„ {attempt+1}/{retries}: {url}")
            page = await browser_ref[0].get(url)
            html = await page.get_content()
            soup = BeautifulSoup(html, "lxml")
            if any(err in soup.get_text().lower() for err in [
                "500", "internal server error", "proxy error", "nginx", "html error", "bad gateway"
            ]):
                raise Exception("ğŸ›‘ HTML ë‚´ ì—ëŸ¬ í˜ì´ì§€ íƒì§€ë¨")
            return page
        except Exception as e:
            print(f"âš ï¸ ë¸Œë¼ìš°ì € ì‘ì—… ì‹¤íŒ¨ {attempt+1}/{retries}: {e}")
            print("ğŸ”„ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì¤‘...")
            try:
                await browser_ref[0].stop()
            except:
                pass
            await asyncio.sleep(delay)
            browser_ref[0] = await start_browser(executable)
    raise Exception("âŒ ë¸Œë¼ìš°ì € ì¬ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨")

def normalize(text: str) -> str:
    if not text:
        return ''
    text = re.sub(r'\s+', '', text)                 # ëª¨ë“  ê³µë°± ì œê±°
    text = re.sub(r'[^\wê°€-í£]', '', text)           # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'[\u200b\u200c\u200d\ufeff\xa0]', '', text)  # ë¹„ê°€ì‹œ ë¬¸ì ì œê±°
    return text.lower()



def extract_rq_items(html_text: str):
    """
    window.__RQ_STREAMING_STATE__.push({...}); ë¸”ë¡ ì•ˆì˜ JSONì„ ì¶”ì¶œí•´ì„œ
    items ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ë¶€ ë°˜í™˜í•œë‹¤.
    """
    parser = BeautifulSoup(html_text, "lxml")
    scripts = parser.find_all("script")
    # pprint.pprint(scripts) # ë””ë²„ê¹…ìš©

    # window.__RQ_STREAMING_STATE__.push(...) ë¥¼ ì°¾ëŠ” ì •ê·œì‹
    # JSON ê°ì²´ê°€ ë³µì¡í•˜ê³  ì—¬ëŸ¬ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê´€ëŒ€í•œ íŒ¨í„´ ì‚¬ìš©
    push_regex = re_compile(
        r'window\.__RQ_STREAMING_STATE__\.push\((.*?)\)\s*;?\s*$', # ëë¶€ë¶„ ê³µë°±/ì„¸ë¯¸ì½œë¡  í—ˆìš©
        re.DOTALL | re.MULTILINE # ì—¬ëŸ¬ ì¤„ì— ê±¸ì³ ë§¤ì¹­
    )

    all_items = []
    found_pushes = 0

    for script in scripts:
        if not script.string:
            continue

        # ì •ê·œì‹ìœ¼ë¡œ push í˜¸ì¶œ ë¶€ë¶„ ì°¾ê¸°
        matches = push_regex.findall(script.string.strip()) # ìŠ¤í¬ë¦½íŠ¸ ë‚´ìš© ì•ë’¤ ê³µë°± ì œê±°
        # pprint.pprint(matches) # ë””ë²„ê¹…ìš©
        for match in matches:
            found_pushes += 1
            # print(f"DEBUG: Found push content: {match[:200]}...") # ë””ë²„ê¹…ìš©
            try:
                # JSON íŒŒì‹± ì‹œë„
                parsed = json.loads(match)

                # 'queries' í‚¤ í™•ì¸ ë° ìˆœíšŒ
                queries = parsed.get("queries", [])
                if not isinstance(queries, list):
                    # print("DEBUG: 'queries' is not a list.")
                    continue

                for q_index, q in enumerate(queries):
                    # 'state', 'data', 'items' ê²½ë¡œ í™•ì¸
                    items = q.get("state", {}).get("data", {}).get("items", [])

                    # itemsê°€ ë¦¬ìŠ¤íŠ¸ì´ê³  ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
                    if isinstance(items, list) and items:
                        print(f"âœ… Script ë‚´ì—ì„œ {len(items)}ê°œì˜ items ë°œê²¬ (query index: {q_index})")
                        all_items.extend(items) # ì°¾ì€ ì•„ì´í…œ ì¶”ê°€

            except json.JSONDecodeError as e:
                # print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}. Content: {match[:300]}...") # ë””ë²„ê¹…ìš©
                continue # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ë§¤ì¹˜ ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë¡œ
            except Exception as e:
                print(f"âš ï¸ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                continue

    if found_pushes == 0:
         print("ğŸŸ¡ RQ_STREAMING_STATE push í˜¸ì¶œì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    elif not all_items:
        print("ğŸŸ¡ push í˜¸ì¶œì€ ì°¾ì•˜ìœ¼ë‚˜, ìœ íš¨í•œ 'items' ë°ì´í„°ë¥¼ í¬í•¨í•œ í˜¸ì¶œì´ ì—†ì—ˆìŠµë‹ˆë‹¤.")
    else:
         print(f"âœ… ìµœì¢… ì¶”ì¶œëœ items: {len(all_items)}ê°œ")

    return all_items

def extract_menu_items_from_apollo(apollo_json):
    menu_items = []

    place_detail_base_key = None
    for key in apollo_json.keys():
        if key.startswith("PlaceDetailBase:"):
            place_detail_base_key = key
            break

    if place_detail_base_key:
        place_detail_base_data = apollo_json.get(place_detail_base_key, {})
        coordinate_data = place_detail_base_data.get("coordinate", {})
        if coordinate_data:
            # Naver often uses 'y' for latitude and 'x' for longitude
            latitude = coordinate_data.get("y")
            longitude = coordinate_data.get("x")
            if latitude is not None and longitude is not None:
                coordinates = {
                    "latitude": str(latitude).strip(), # Ensure it's a string
                    "longitude": str(longitude).strip() # Ensure it's a string
                }
                print(f"âœ… ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ ì„±ê³µ: {coordinates}")
            else:
                print("ğŸŸ¡ PlaceDetailBaseì— y ë˜ëŠ” x ì¢Œí‘œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print("ğŸŸ¡ PlaceDetailBaseì— coordinate ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("ğŸŸ¡ APOLLO_STATEì—ì„œ PlaceDetailBase ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì¢Œí‘œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


    for key, value in apollo_json.items():
        if key.startswith("Menu:") and isinstance(value, dict):
            menu_data = {
                "name": value.get("name", "").strip(),
                "price": value.get("price", "").strip(),
                "description": value.get("description", "").strip(),
                "images": value.get("images", [])
            }
            menu_items.append(menu_data)

    return menu_items, coordinates


async def crawler():
    restaurant_infos = load_10_restaurant_names_and_addresses()

    if not restaurant_infos:
        print("âŒ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ê²Œ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    print(f"â„¹ï¸ {len(restaurant_infos)}ê°œ ê°€ê²Œì— ëŒ€í•œ í¬ë¡¤ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    system = platform.platform()
    arch = platform.machine()
    executable = None

    print("ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸")
    print(f"ì‹œìŠ¤í…œ: {system}")
    print(f"ì•„í‚¤í…ì²˜: {arch}")

    if system != "mac" and arch in ("aarch64", "arm64"):
        print("ARM64 í™˜ê²½ ê°ì§€")
        if os.path.exists("/usr/bin/ungoogled-chromium"):
            executable = "/usr/bin/ungoogled-chromium"
        elif os.path.exists("/usr/bin/chromium"):
            executable = "/usr/bin/chromium"

    try:
        browser_ref = [await start_browser(executable)]

        print("âœ… Zendriver ì‹œì‘ ì™„ë£Œ.")
        success, fail, need_check = 0, 0, 0

        for index, (id, business_name, naver_id) in enumerate(restaurant_infos):
            if (index + 1) % 50 == 0:
                await page.close()
                await browser_ref[0].stop()
                print("ğŸ”„ ë©”ëª¨ë¦¬ ìœ ì¶œ ë°©ì§€ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì¤‘...")
                browser_ref = [await start_browser(executable)]
                print("âœ… ë©”ëª¨ë¦¬ ìœ ì¶œ ë°©ì§€ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì™„ë£Œ.")

            try:
                #search_query = make_search_query(business_name, road_address)
                search_query = re.sub(r'\D', '', naver_id).strip()

                print(f"ğŸ” [{index + 1} | {len(restaurant_infos)}] ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
                encoded_query = urllib.parse.quote(search_query)
                mob_url = f"https://m.place.naver.com/place/{encoded_query}/menu"
                print(f"ğŸ”— [{index + 1} | {len(restaurant_infos)}] {business_name}")
                print(f"ğŸ”— [{index + 1} | {len(restaurant_infos)}] {search_query} URL: {mob_url}")

                page = await with_browser_get(mob_url, browser_ref, executable, retries=5, delay=3)
                await page.wait_for("div.place_fixed_maintab", timeout=10)
                html_src = await page.get_content()
                soup = BeautifulSoup(html_src, 'lxml')

                scripts = soup.find_all('script')
                apollo_data_raw = None
                for script in scripts:
                    # Check if the script content exists and contains the target string
                    if script.string and "window.__APOLLO_STATE__" in script.string:
                        # --- Corrected Regex ---
                        # Removed the '$' anchor to match even if other JS code follows the APOLLO_STATE assignment.
                        # Added non-greedy match {.*?} just in case of nested structures, though {.*} often works too.
                        match = re.search(r"window\.__APOLLO_STATE__\s*=\s*({.*?});", script.string, re.DOTALL)
                        if match:
                            apollo_data_raw = match.group(1)
                            print("âœ… APOLLO_STATE ìŠ¤í¬ë¦½íŠ¸ ë¸”ë¡ ì°¾ìŒ.")
                            break  # Exit the loop once found

                if not apollo_data_raw:
                    print("ğŸŸ¡ APOLLO_STATE ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    return []

                try:
                    # Attempt to load the extracted string as JSON
                    apollo_json = json.loads(apollo_data_raw)
                    print("âœ… APOLLO_STATE JSON íŒŒì‹± ì„±ê³µ.")
                    #pprint.pprint(apollo_json)  # Optional: Print the parsed JSON for debugging
                except json.JSONDecodeError as e:
                    print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                    # Optional: Print a snippet of the raw data for debugging JSON errors
                    # print("--- íŒŒì‹± ì‹œë„í•œ ë°ì´í„° (ì¼ë¶€) ---")
                    # print(apollo_data_raw[:500] + "..." if apollo_data_raw else "N/A")
                    # print("-----------------------------")
                    return []

                menu_items, cordinates = extract_menu_items_from_apollo(apollo_json)

                data = {
                    "id": id,
                    "cordinates": cordinates,
                    "query": search_query,
                    "title": business_name,
                    "menu": menu_items,
                    "url": mob_url
                }
                # print(data)

                append_to_json_file(data, output_path)
                success += 1

            except Exception as e:
                print(f"âŒ [{index + 1} | {len(restaurant_infos)}] JSON ë§¤ì¹­ ì‹¤íŒ¨: {e}")
                fail += 1
                continue


    finally:
        await browser_ref[0].stop()
        print("ğŸ›‘ Zendriver ì¢…ë£Œ ì™„ë£Œ")
        print("ğŸ›‘ í¬ë¡¤ëŸ¬ ì¢…ë£Œ ì™„ë£Œ")
        print(f"\nâœ… ì™„ë£Œ: {success} / âŒ ì‹¤íŒ¨: {fail} / âš ï¸ í™•ì¸ í•„ìš”: {need_check}")


if __name__ == "__main__":
    #store_first_db()
    if not os.path.exists("screenshots"):
        os.makedirs("screenshots")

    if not os.path.exists("web_data"):
        os.makedirs("web_data")

    if not os.path.exists("error_logs"):
        os.makedirs("error_logs")

    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    SCREENSHOT_DIR = os.path.join(BASE_DIR, "screenshots")
    DATA_DIR = os.path.join(BASE_DIR, "web_data")
    ERROR_DIR = os.path.join(BASE_DIR, "error_logs")

    os.makedirs(SCREENSHOT_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(ERROR_DIR, exist_ok=True)

    print("ğŸ“‚ Screenshot ì €ì¥ ê²½ë¡œ:", SCREENSHOT_DIR)
    print("ğŸ“‚ WebData ì €ì¥ ê²½ë¡œ:", DATA_DIR)
    print("ğŸ“‚ ErrorLog ì €ì¥ ê²½ë¡œ:", ERROR_DIR)

    start_index = int(os.environ.get("START_INDEX", sys.argv[1] if len(sys.argv) > 1 else 0))
    output_path = os.path.join(DATA_DIR, f"crawl_menu_{start_index}.json")

    browser_args = [
        "--no-sandbox",
        "--disable-dev-shm-usage",
        "--disable-setuid-sandbox",
        "--disable-software-rasterizer",
        "--disable-blink-features=AutomationControlled",
        "--window-size=1280x800",  # ë°˜ë“œì‹œ ì‚¬ì´ì¦ˆ ì§€ì •
        # "--start-maximized",  # headlessì—ì„œë„ ìµœëŒ€í™”ì²˜ëŸ¼ ë³´ì´ê²Œ
        "--disable-infobars",
        "--disable-extensions",
        "--disable-popup-blocking",
        "--enable-logging=stderr",
        "--log-level=1",
    ]

    asyncio.run(crawler())