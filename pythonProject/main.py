import asyncio
import json
import random
import time
from time import sleep
import lxml
from bs4 import BeautifulSoup
import chardet
import pandas as pd
import pprint
import sqlite3
import zendriver as zd
import urllib
import re
import os

if not os.path.exists("screenshots"):
    os.makedirs("screenshots")

def store_first_db():

    file_path = 'fulldata_07_24_04_P_ì¼ë°˜ìŒì‹ì .csv'

    with open(file_path, 'rb') as f:
        sample = f.read(1024 * 1024)  # 1MB

    # ìƒ˜í”Œ ë°ì´í„°ë¡œ ì¸ì½”ë”© ê°ì§€
    encoding_data = chardet.detect(sample)
    detected_encoding = encoding_data['encoding']
    print(f"Detected encoding: {detected_encoding}")

    data = pd.read_csv(file_path, encoding=detected_encoding, low_memory=False)
    pprint.pprint(data.head())
    print("Data loaded successfully.")

    # for column in data.columns:
    #     unique_values = data[column].dropna().unique()
    #     print(f"ğŸ“Œ ì»¬ëŸ¼: {column}")
    #     print(f"   â–¶ ê³ ìœ ê°’ {len(unique_values)}ê°œ")
    #     print(f"   â–¶ ìƒ˜í”Œ: {unique_values[:10]}")  # ì²˜ìŒ 10ê°œë§Œ ë³´ì—¬ì¤Œ
    #     print("-" * 50)

    #check data length
    print(f"Total records: {len(data)}")

    # íì—… ì œì™¸
    if 'ì˜ì—…ìƒíƒœëª…' in data.columns:
        print("ğŸ› ï¸ 'ì˜ì—…ìƒíƒœëª…' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ íì—… ë°ì´í„° ê±¸ëŸ¬ë‚´ëŠ” ì¤‘...")
        data = data[data['ì˜ì—…ìƒíƒœëª…'] != 'íì—…']
    else:
        print("âš ï¸ 'ì˜ì—…ìƒíƒœëª…' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. íì—… ë°ì´í„° ê±¸ëŸ¬ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    conn = sqlite3.connect('food_data.db')

    # DataFrameì„ SQLite í…Œì´ë¸”ë¡œ ì €ì¥
    data.to_sql('restaurants', conn, if_exists='replace', index=False)

    # ì—°ê²° ëŠê¸°
    conn.close()

    print(f"âœ… íì—… ì œì™¸ í›„ {len(data)}ê±´ ì €ì¥ ì™„ë£Œ! (DB: food_data.db, Table: restaurants)")

def load_10_restaurant_names_and_addresses():
    conn = sqlite3.connect('food_data.db')
    cursor = conn.cursor()

    cursor.execute("SELECT ì‚¬ì—…ì¥ëª…, ë„ë¡œëª…ì „ì²´ì£¼ì†Œ FROM restaurants ;")
    rows = cursor.fetchall()

    conn.close()

    restaurant_infos = []
    for row in rows:
        business_name, road_address = row
        if business_name and road_address:
            restaurant_infos.append((business_name, road_address))
    return restaurant_infos

def make_search_query(business_name, road_address):
    # ë„ë¡œëª… ì£¼ì†Œ ì• 3ë‹¨ê³„ê¹Œì§€ë§Œ
    parts = road_address.split()
    if len(parts) >= 3:
        filtered_address = ' '.join(parts[:3])
    else:
        filtered_address = road_address

    query = f"{business_name} {filtered_address}"
    # pprint.pprint(f"Search query: {query}")
    return query

def sanitize_filename(name):
    # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±° (\ / * ? : " < > |)
    name = re.sub(r'[\\/*?:"<>|]', "", name)
    # ê³µë°±ì„ ë°‘ì¤„ë¡œ ë³€ê²½ (ì„ íƒ ì‚¬í•­)
    name = name.replace(" ", "_")
    # í•„ìš”ì‹œ ìµœëŒ€ ê¸¸ì´ ì œí•œ (ì„ íƒ ì‚¬í•­)
    max_len = 100
    if len(name) > max_len:
        name = name[:max_len]
    return name


def extract_menu_data_from_html(html_content):
    """
    ì£¼ì–´ì§„ HTML ë¬¸ìì—´ì—ì„œ window.__APOLLO_STATE__ë¥¼ ì°¾ì•„ ë©”ë‰´ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    menu_items = []
    processed_menu_names = set() # ê°„ë‹¨í•œ ë©”ë‰´ ì´ë¦„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°ìš©

    # ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ __APOLLO_STATE__ JSON ë¬¸ìì—´ ì°¾ê¸°
    # ì„¸ë¯¸ì½œë¡ (;)ì´ ë’¤ì— ì˜¤ëŠ” íŒ¨í„´ì„ ëª…í™•íˆ í•¨
    match = re.search(r'window\.__APOLLO_STATE__\s*=\s*(\{.*?\});', html_content, re.DOTALL)
    if not match:
        print("âš ï¸ ê²½ê³ : HTMLì—ì„œ window.__APOLLO_STATE__ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return menu_items # ë°ì´í„° ëª» ì°¾ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    apollo_state_str = match.group(1)

    try:
        # JSON íŒŒì‹±
        apollo_state = json.loads(apollo_state_str)
    except json.JSONDecodeError as e:
        print(f"âŒ ì˜¤ë¥˜: __APOLLO_STATE__ JSON íŒŒì‹± ì‹¤íŒ¨ - {e}")
        # íŒŒì‹± ì˜¤ë¥˜ ì‹œ ë””ë²„ê¹… ì •ë³´ ì¶”ê°€ ê°€ëŠ¥
        # error_pos = e.pos
        # context_len = 50
        # start = max(0, error_pos - context_len)
        # end = min(len(apollo_state_str), error_pos + context_len)
        # print(f"ì—ëŸ¬ ë°œìƒ ìœ„ì¹˜ ê·¼ì²˜: ...{apollo_state_str[start:end]}...")
        return menu_items # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    # Apollo State ë”•ì…”ë„ˆë¦¬ ìˆœíšŒí•˜ë©° ë©”ë‰´ ì •ë³´ ì¶”ì¶œ
    for key, value in apollo_state.items():
        # valueê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì´ê³ , __typename í‚¤ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
        if isinstance(value, dict):
            typename = value.get("__typename")

            # ë©”ë‰´ ê´€ë ¨ íƒ€ì…ì¸ì§€ í™•ì¸ (ì´ íƒ€ì… ì´ë¦„ë“¤ì€ ì‹¤ì œ ë°ì´í„° í™•ì¸ í›„ ì¡°ì • í•„ìš”)
            if typename == "Menu" or typename == "PlaceDetail_BaeminMenu":
                menu_name = value.get("name")
                menu_price = value.get("price")
                # ì„¤ëª… í•„ë“œëŠ” 'desc' ë˜ëŠ” 'description'ì¼ ìˆ˜ ìˆìŒ
                menu_desc = value.get("desc", value.get("description", ""))
                images = value.get("images", [])

                if menu_name and menu_price is not None: # ì´ë¦„ê³¼ ê°€ê²©ì´ ëª¨ë‘ ìˆì–´ì•¼ í•¨
                    # ê°€ê²© ì •ë¦¬ (ìˆ«ìë§Œ ì¶”ì¶œ)
                    # ê°€ê²©ì´ ì´ë¯¸ ìˆ«ìì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ str()ë¡œ ë³€í™˜ í›„ ì •ê·œì‹ ì ìš©
                    cleaned_price_str = re.sub(r'[^0-9]', '', str(menu_price))
                    cleaned_price = int(cleaned_price_str) if cleaned_price_str else None

                    # ê°„ë‹¨í•˜ê²Œ ë©”ë‰´ ì´ë¦„ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬ (ë” ì •êµí•œ ë¡œì§ ê°€ëŠ¥)
                    if menu_name not in processed_menu_names:
                        menu_items.append({
                            "name": menu_name,
                            "price": cleaned_price,
                            "description": menu_desc,
                            "images": images
                        })
                        processed_menu_names.add(menu_name)

    return menu_items

async def crawler():
    restaurant_infos = load_10_restaurant_names_and_addresses()
    if not restaurant_infos:
        print("âŒ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ê²Œ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    print(f"â„¹ï¸ {len(restaurant_infos)}ê°œ ê°€ê²Œì— ëŒ€í•œ í¬ë¡¤ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    if not os.path.exists("screenshots"):
        os.makedirs("screenshots")

    try:
        print("ğŸš€ Zendriver ì‹œì‘ ì¤‘...")
        browser = await zd.start(headless=True)
        print("âœ… Zendriver ì‹œì‘ ì™„ë£Œ. ë¸Œë¼ìš°ì € ê°ì²´ í™•ë³´.")

        results = []

        for index, (business_name, road_address) in enumerate(restaurant_infos):
            search_query = make_search_query(business_name, road_address)
            encoded_query = urllib.parse.quote(search_query) # URL ì¸ì½”ë”©
            search_url = f"https://map.naver.com/p/search/{encoded_query}"

            print(f"\n--- {index+1}/{len(restaurant_infos)} ì²˜ë¦¬ ì¤‘: {search_query} ---")
            print(f"ğŸ”— URL: {search_url}")

            try:
                # 1. browser.get()ìœ¼ë¡œ ì´ë™í•˜ê³ , ë°˜í™˜ëœ page/tab ê°ì²´ë¥¼ ì‚¬ìš©
                page = await browser.get(search_url)
                print("ğŸŒ í˜ì´ì§€ ì´ë™ ì™„ë£Œ. ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° ì¤‘...") # browser.getì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦°ë‹¤ê³  ê°€ì •

                # --- ì´ì œ 'page' (Tab ê°ì²´)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì†Œ ì°¾ê¸° ---
                search_iframe_selector = "#searchIframe"
                entry_iframe_selector = "#entryIframe"

                pprint.pprint("ğŸ”„ searchIframe ë¡œë”© ëŒ€ê¸°ì¤‘...")
                await page.wait_for(search_iframe_selector, timeout=10000)
                pprint.pprint("âœ… searchIframe ë¡œë”© ì™„ë£Œ.")

                # tmp_content = await page.get_content()
                pprint.pprint('SearchIframe url í™•ì¸ì¤‘')
                iframe_elements_str_list = await page.select_all("#searchIframe")
                iframe_string = str(iframe_elements_str_list[0])
                match = re.search(r'src="([^"]*)"', iframe_string)
                pprint.pprint(f"iframe URL: {match.group(1)}")

                await page.get(match.group(1))
                tmp_content = await page.get_content()
                tmp_parse = BeautifulSoup(tmp_content, "lxml")

                if tmp_parse.select("div[class='FYvSc']"):
                    pprint.pprint("âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ.")
                    continue

                await page.get(search_url)
                pprint.pprint("ğŸ”„ entryIframe ë¡œë”© ëŒ€ê¸°ì¤‘...")
                # await page.wait_for(search_iframe_selector, timeout=10000)
                await page.wait_for(entry_iframe_selector, timeout=10)
                pprint.pprint("âœ… entryIframe ë¡œë”© ì™„ë£Œ.")
                iframe_elements_str_list = await page.select_all("#entryIframe")
                iframe_string = str(iframe_elements_str_list[0])
                match = re.search(r'src="([^"]*)"', iframe_string)
                pprint.pprint(f"iframe URL: {match.group(1)}")
                await page.get(match.group(1))
                content = await page.get_content()
                soup = BeautifulSoup(content, "lxml")
                extracted_menu_list = extract_menu_data_from_html(content)
                print(f"ğŸ½ï¸ ì¶”ì¶œëœ ë©”ë‰´ ê°œìˆ˜: {len(extracted_menu_list)}")
                if extracted_menu_list:
                    pprint.pprint(f"ìƒ˜í”Œ ë©”ë‰´: {extracted_menu_list[:3]}")  # ì²˜ìŒ 3ê°œ ë©”ë‰´ ìƒ˜í”Œ ì¶œë ¥

                title = soup.title.string if soup.title else business_name  # ì œëª© ì—†ìœ¼ë©´ ê°€ê²Œ ì´ë¦„ ì‚¬ìš©
                data = {
                    "title": title,
                    "meta_description": soup.find("meta", {"name": "description"})["content"]
                    if soup.find("meta", {"name": "description"}) else "No Description",
                    "extracted_menus": extracted_menu_list,  # ì¶”ì¶œëœ ë©”ë‰´ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
                    "raw_html": content  # ì›ë³¸ HTMLë„ í•„ìš”í•˜ë©´ ìœ ì§€
                }

                filename = sanitize_filename(f"{business_name}{time.time()}.json")
                with open(f"web_data/{filename}", "w", encoding="utf-8") as json_file:
                    json.dump(data, json_file, ensure_ascii=False, indent=4)


            except Exception as e:
                print(f"âŒ iframe ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

        # --- ëª¨ë“  ë£¨í”„ ì¢…ë£Œ í›„ ---
        print("\nğŸ‰ í¬ë¡¤ëŸ¬ ì‘ì—… ì™„ë£Œ.")

    except Exception as start_err:
        print(f"ğŸ’¥ Zendriver ì‹œì‘ ë˜ëŠ” ì „ì²´ í¬ë¡¤ë§ ê³¼ì •ì—ì„œ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {start_err}")

if __name__ == "__main__":
    #store_first_db()
    asyncio.run(crawler())