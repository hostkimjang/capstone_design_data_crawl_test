import asyncio
import json
import random
import time
from asyncio import wait_for
from time import sleep
import lxml
import platform
from bs4 import BeautifulSoup
import chardet
import pandas as pd
import pprint
import sqlite3
import zendriver as zd
import urllib
import re
import os

if not os.path.exists("screenshots"):
    os.makedirs("screenshots")

if not os.path.exists("web_data"):
    os.makedirs("web_data")


def store_first_db():

    file_path = 'fulldata_07_24_04_P_ì¼ë°˜ìŒì‹ì .csv'

    with open(file_path, 'rb') as f:
        sample = f.read(1024 * 1024)  # 1MB

    # ìƒ˜í”Œ ë°ì´í„°ë¡œ ì¸ì½”ë”© ê°ì§€
    encoding_data = chardet.detect(sample)
    detected_encoding = encoding_data['encoding']
    print(f"Detected encoding: {detected_encoding}")

    data = pd.read_csv(file_path, encoding=detected_encoding, low_memory=False)
    pprint.pprint(data.head())
    print("Data loaded successfully.")

    # for column in data.columns:
    #     unique_values = data[column].dropna().unique()
    #     print(f"ğŸ“Œ ì»¬ëŸ¼: {column}")
    #     print(f"   â–¶ ê³ ìœ ê°’ {len(unique_values)}ê°œ")
    #     print(f"   â–¶ ìƒ˜í”Œ: {unique_values[:10]}")  # ì²˜ìŒ 10ê°œë§Œ ë³´ì—¬ì¤Œ
    #     print("-" * 50)

    #check data length
    print(f"Total records: {len(data)}")

    # íì—… ì œì™¸
    if 'ì˜ì—…ìƒíƒœëª…' in data.columns:
        print("ğŸ› ï¸ 'ì˜ì—…ìƒíƒœëª…' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ íì—… ë°ì´í„° ê±¸ëŸ¬ë‚´ëŠ” ì¤‘...")
        data = data[data['ì˜ì—…ìƒíƒœëª…'] != 'íì—…']
    else:
        print("âš ï¸ 'ì˜ì—…ìƒíƒœëª…' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. íì—… ë°ì´í„° ê±¸ëŸ¬ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    conn = sqlite3.connect('food_data.db')

    # DataFrameì„ SQLite í…Œì´ë¸”ë¡œ ì €ì¥
    data.to_sql('restaurants', conn, if_exists='replace', index=False)

    # ì—°ê²° ëŠê¸°
    conn.close()

    print(f"âœ… íì—… ì œì™¸ í›„ {len(data)}ê±´ ì €ì¥ ì™„ë£Œ! (DB: food_data.db, Table: restaurants)")

def load_10_restaurant_names_and_addresses():
    conn = sqlite3.connect('food_data.db')
    cursor = conn.cursor()

    cursor.execute("SELECT ì‚¬ì—…ì¥ëª…, ë„ë¡œëª…ì „ì²´ì£¼ì†Œ FROM restaurants LIMIT 50;")
    rows = cursor.fetchall()

    conn.close()

    restaurant_infos = []
    for row in rows:
        business_name, road_address = row
        if business_name and road_address:
            restaurant_infos.append((business_name, road_address))
    return restaurant_infos

def make_search_query(business_name, road_address):
    # ë„ë¡œëª… ì£¼ì†Œ ì• 3ë‹¨ê³„ê¹Œì§€ë§Œ
    parts = road_address.split()
    if len(parts) >= 3:
        filtered_address = ' '.join(parts[:3])
    else:
        filtered_address = road_address

    query = f"{business_name} {filtered_address}"
    # pprint.pprint(f"Search query: {query}")
    return query

def sanitize_filename(name):
    # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±° (\ / * ? : " < > |)
    name = re.sub(r'[\\/*?:"<>|]', "", name)
    # ê³µë°±ì„ ë°‘ì¤„ë¡œ ë³€ê²½ (ì„ íƒ ì‚¬í•­)
    name = name.replace(" ", "_")
    # í•„ìš”ì‹œ ìµœëŒ€ ê¸¸ì´ ì œí•œ (ì„ íƒ ì‚¬í•­)
    max_len = 100
    if len(name) > max_len:
        name = name[:max_len]
    return name


def extract_menu_data_from_html(html_content):
    """
    ì£¼ì–´ì§„ HTML ë¬¸ìì—´ì—ì„œ window.__APOLLO_STATE__ë¥¼ ì°¾ì•„ ë©”ë‰´ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    menu_items = []
    processed_menu_names = set() # ê°„ë‹¨í•œ ë©”ë‰´ ì´ë¦„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°ìš©

    # ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ __APOLLO_STATE__ JSON ë¬¸ìì—´ ì°¾ê¸°
    # ì„¸ë¯¸ì½œë¡ (;)ì´ ë’¤ì— ì˜¤ëŠ” íŒ¨í„´ì„ ëª…í™•íˆ í•¨
    match = re.search(r'window\.__APOLLO_STATE__\s*=\s*(\{.*?\});', html_content, re.DOTALL)
    if not match:
        print("âš ï¸ ê²½ê³ : HTMLì—ì„œ window.__APOLLO_STATE__ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return menu_items # ë°ì´í„° ëª» ì°¾ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    apollo_state_str = match.group(1)

    try:
        # JSON íŒŒì‹±
        apollo_state = json.loads(apollo_state_str)
    except json.JSONDecodeError as e:
        print(f"âŒ ì˜¤ë¥˜: __APOLLO_STATE__ JSON íŒŒì‹± ì‹¤íŒ¨ - {e}")
        # íŒŒì‹± ì˜¤ë¥˜ ì‹œ ë””ë²„ê¹… ì •ë³´ ì¶”ê°€ ê°€ëŠ¥
        # error_pos = e.pos
        # context_len = 50
        # start = max(0, error_pos - context_len)
        # end = min(len(apollo_state_str), error_pos + context_len)
        # print(f"ì—ëŸ¬ ë°œìƒ ìœ„ì¹˜ ê·¼ì²˜: ...{apollo_state_str[start:end]}...")
        return menu_items # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    # Apollo State ë”•ì…”ë„ˆë¦¬ ìˆœíšŒí•˜ë©° ë©”ë‰´ ì •ë³´ ì¶”ì¶œ
    for key, value in apollo_state.items():
        # valueê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì´ê³ , __typename í‚¤ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
        if isinstance(value, dict):
            typename = value.get("__typename")

            # ë©”ë‰´ ê´€ë ¨ íƒ€ì…ì¸ì§€ í™•ì¸ (ì´ íƒ€ì… ì´ë¦„ë“¤ì€ ì‹¤ì œ ë°ì´í„° í™•ì¸ í›„ ì¡°ì • í•„ìš”)
            if typename == "Menu" or typename == "PlaceDetail_BaeminMenu":
                menu_name = value.get("name")
                menu_price = value.get("price")
                # ì„¤ëª… í•„ë“œëŠ” 'desc' ë˜ëŠ” 'description'ì¼ ìˆ˜ ìˆìŒ
                menu_desc = value.get("desc", value.get("description", ""))
                images = value.get("images", [])

                if menu_name and menu_price is not None: # ì´ë¦„ê³¼ ê°€ê²©ì´ ëª¨ë‘ ìˆì–´ì•¼ í•¨
                    # ê°€ê²© ì •ë¦¬ (ìˆ«ìë§Œ ì¶”ì¶œ)
                    # ê°€ê²©ì´ ì´ë¯¸ ìˆ«ìì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ str()ë¡œ ë³€í™˜ í›„ ì •ê·œì‹ ì ìš©
                    cleaned_price_str = re.sub(r'[^0-9]', '', str(menu_price))
                    cleaned_price = int(cleaned_price_str) if cleaned_price_str else None

                    # ê°„ë‹¨í•˜ê²Œ ë©”ë‰´ ì´ë¦„ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬ (ë” ì •êµí•œ ë¡œì§ ê°€ëŠ¥)
                    if menu_name not in processed_menu_names:
                        menu_items.append({
                            "name": menu_name,
                            "price": cleaned_price,
                            "description": menu_desc,
                            "images": images
                        })
                        processed_menu_names.add(menu_name)

    return menu_items


def extract_dynamic_place_info(soup: BeautifulSoup):
    place_info = {}

    info_blocks = soup.select("div.PIbes > div.O8qbU")

    for block in info_blocks:
        key_elem = block.select_one("strong > span.place_blind")
        value_block = block.select_one("div.vV_z_")
        if not key_elem or not value_block:
            continue

        key = key_elem.get_text(strip=True)

        if key == "ì£¼ì†Œ":
            addr = value_block.select_one("span.LDgIH")
            place_info["ì£¼ì†Œ"] = addr.text.strip() if addr else None

        elif key == "ì „í™”ë²ˆí˜¸":
            phone = value_block.select_one("span.xlx7Q")
            place_info["ì „í™”ë²ˆí˜¸"] = phone.text.strip() if phone else None

        elif key == "ì˜ì—…ì‹œê°„":
            status = value_block.select_one("em")
            hours = value_block.select_one("time")
            place_info["ì˜ì—…ìƒíƒœ"] = status.text.strip() if status else None
            place_info["ì˜ì—…ì‹œê°„"] = hours.text.strip() if hours else None

        elif key == "í™ˆí˜ì´ì§€":
            links = value_block.select("a.place_bluelink")
            place_info["í™ˆí˜ì´ì§€ë“¤"] = [a["href"] for a in links if a.get("href")]

        else:
            place_info[key] = value_block.get_text(strip=True)

    return place_info

async def wait_for_selector_with_retry(page, selector, timeout=10, interval=1):
    max_attempts = int(timeout / interval)
    for attempt in range(max_attempts):
        try:
            node = await page.select_all(selector)
            if node:
                return node
        except Exception:
            pass
        await asyncio.sleep(interval)
    raise TimeoutError(f"âŒ '{selector}' ë¡œë”© ì‹¤íŒ¨ (timeout={timeout}s)")

async def crawler():
    restaurant_infos = load_10_restaurant_names_and_addresses()
    if not restaurant_infos:
        print("âŒ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ê²Œ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    print(f"â„¹ï¸ {len(restaurant_infos)}ê°œ ê°€ê²Œì— ëŒ€í•œ í¬ë¡¤ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    if not os.path.exists("screenshots"):
        os.makedirs("screenshots")

    system = platform.platform()
    arch = platform.machine()
    executable = None

    print("ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸")
    print(f"ì‹œìŠ¤í…œ: {system}")
    print(f"ì•„í‚¤í…ì²˜: {arch}")

    if system != "mac" and arch in ("aarch64", "arm64"):
        print("ARM64 í™˜ê²½ ê°ì§€")
        if os.path.exists("/usr/bin/ungoogled-chromium"):
            executable = "/usr/bin/ungoogled-chromium"
        elif os.path.exists("/usr/bin/chromium"):
            executable = "/usr/bin/chromium"

    try:
        print("ğŸš€ Zendriver ì‹œì‘ ì¤‘...")
        print(f" ë¸Œë¼ìš°ì € ì‹¤í–‰ ìœ„ì¹˜. {executable}")
        browser = await zd.start(
            browser_executable_path=executable,
        )
        print("âœ… Zendriver ì‹œì‘ ì™„ë£Œ. ë¸Œë¼ìš°ì € ê°ì²´ í™•ë³´.")

        results = []

        for index, (business_name, road_address) in enumerate(restaurant_infos):
            search_query = make_search_query(business_name, road_address)
            encoded_query = urllib.parse.quote(search_query) # URL ì¸ì½”ë”©
            search_url = f"https://map.naver.com/p/search/{encoded_query}"
            mob_search_url = f"https://m.place.naver.com/restaurant/list?query={encoded_query}&x=126&y=37"

            print(f"\n--- {index+1}/{len(restaurant_infos)} ì²˜ë¦¬ ì¤‘: {search_query} ---")
            print(f"ğŸ”— URL: {mob_search_url}")

            try:
                # 1. browser.get()ìœ¼ë¡œ ì´ë™í•˜ê³ , ë°˜í™˜ëœ page/tab ê°ì²´ë¥¼ ì‚¬ìš©
                page = await browser.get(mob_search_url)
                print("ğŸŒ í˜ì´ì§€ ì´ë™ ì™„ë£Œ. ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° ì¤‘...") # browser.getì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦°ë‹¤ê³  ê°€ì •

                # --- ì´ì œ 'page' (Tab ê°ì²´)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì†Œ ì°¾ê¸° ---
                place_business_list_wrapper = "div.place_business_list_wrapper"
                place_fixed_maintab_selector = "div.place_fixed_maintab"

                pprint.pprint("ğŸ”„ place_business_list_wrapper ë¡œë”© ëŒ€ê¸°ì¤‘...")
                # 1. ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ëŒ€ê¸° ì‹œë„
                try:
                    await page.wait_for("div.place_business_list_wrapper", timeout=5)
                    print("âœ… ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ.")

                    tmp_content = await page.get_content()
                    soup = BeautifulSoup(tmp_content, "lxml")

                    a_tags = soup.select("div.place_business_list_wrapper > ul > li a[href]")
                    href_list = [a['href'] for a in a_tags]
                    valid_links = [
                        re.match(r"^/restaurant/\d+", href).group(0)
                        for href in href_list if re.match(r"^/restaurant/\d+", href)
                    ]
                    unique_links = list(set(valid_links))

                    if not unique_links:
                        raise Exception("âŒ ë¦¬ìŠ¤íŠ¸ëŠ” ìˆëŠ”ë° ë§í¬ê°€ ì—†ìŒ.")

                    # ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ë§í¬ë¡œ ì´ë™
                    target_url = f"https://m.place.naver.com{unique_links[0]}"
                    print(f"ğŸ” ë¦¬ìŠ¤íŠ¸ì—ì„œ ì²« ë²ˆì§¸ ë§í¬ë¡œ ì´ë™: {target_url}")
                    await page.get(target_url)

                except Exception as e:
                    print(f"âš ï¸ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ê°€ ì—†ê±°ë‚˜ ìë™ ë¦¬ë””ë ‰ì…˜ë¨: {e}")
                    print("ğŸ“Œ í˜„ì¬ í˜ì´ì§€ë¥¼ ìƒì„¸ í˜ì´ì§€ë¡œ ê°„ì£¼í•˜ê³  ì²˜ë¦¬ ê³„ì†í•¨.")

                unique_links = list(set(valid_links))

                print(f"ğŸ½ï¸ ìœ íš¨í•œ ë§í¬ ê°œìˆ˜: {len(unique_links)}")
                print(f"ğŸ½ï¸ ìƒ˜í”Œ ë§í¬: {unique_links[:3]}")

                await page.get(f"https://m.place.naver.com{unique_links[0]}")

                pprint.pprint("ğŸ”„ place_fixed_maintab ë¡œë”© ëŒ€ê¸°ì¤‘...")
                await wait_for_selector_with_retry(page, place_fixed_maintab_selector, timeout=15)
                pprint.pprint("âœ… place_fixed_maintab_selector ë¡œë”© ì™„ë£Œ.")

                parser = BeautifulSoup(await page.get_content(), "lxml")
                main_tab = parser.select_one('div[class="place_fixed_maintab"]')
                if main_tab:
                    href_list = [
                        a['href']
                        for a in main_tab.select('a[href]')
                        if a['href'].strip() and not a['href'].strip().startswith('#')
                    ]
                    print(f"ğŸ½ï¸ ìœ íš¨í•œ ë§í¬ ê°œìˆ˜: {len(href_list)}")
                    print(f"ğŸ½ï¸ ë§í¬: {href_list}")
                else:
                    print("âŒ place_fixed_maintab not found.")

                place_info = extract_dynamic_place_info(parser)
                print(place_info)

                # iframe_elements_str_list = await page.select_all("#entryIframe")
                # iframe_string = str(iframe_elements_str_list[0])
                # match = re.search(r'src="([^"]*)"', iframe_string)
                # pprint.pprint(f"iframe URL: {match.group(1)}")
                # await page.get(match.group(1))
                # content = await page.get_content()
                # soup = BeautifulSoup(content, "lxml")
                # extracted_menu_list = extract_menu_data_from_html(content)
                # print(f"ğŸ½ï¸ ì¶”ì¶œëœ ë©”ë‰´ ê°œìˆ˜: {len(extracted_menu_list)}")
                # if extracted_menu_list:
                #     pprint.pprint(f"ìƒ˜í”Œ ë©”ë‰´: {extracted_menu_list[:3]}")  # ì²˜ìŒ 3ê°œ ë©”ë‰´ ìƒ˜í”Œ ì¶œë ¥
                #
                # title = soup.title.string if soup.title else business_name  # ì œëª© ì—†ìœ¼ë©´ ê°€ê²Œ ì´ë¦„ ì‚¬ìš©
                #data = {
                #     "title": title,
                #     "meta_description": soup.find("meta", {"name": "description"})["content"]
                #     if soup.find("meta", {"name": "description"}) else "No Description",
                #     "extracted_menus": extracted_menu_list,  # ì¶”ì¶œëœ ë©”ë‰´ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
                #     "raw_html": content  # ì›ë³¸ HTMLë„ í•„ìš”í•˜ë©´ ìœ ì§€
                # }
                #
                # filename = sanitize_filename(f"{business_name}{time.time()}.json")
                # with open(f"web_data/{filename}", "w", encoding="utf-8") as json_file:
                #     json.dump(data, json_file, ensure_ascii=False, indent=4)

                data = {
                    "title": business_name,
                    "place_info": place_info,
                    "tab_list": href_list,
                }

                filename = sanitize_filename(f"{business_name}{time.time()}.json")
                with open(f"web_data/{filename}", "w", encoding="utf-8") as json_file:
                    json.dump(data, json_file, ensure_ascii=False, indent=4)

            except Exception as e:
                print(f"âŒ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                await page.save_screenshot(f"screenshots/error_{index+1}_{business_name}_{road_address}.png")
                continue

        # --- ëª¨ë“  ë£¨í”„ ì¢…ë£Œ í›„ ---
        print("\nğŸ‰ í¬ë¡¤ëŸ¬ ì‘ì—… ì™„ë£Œ.")

    except Exception as start_err:
        print(f"ğŸ’¥ Zendriver ì‹œì‘ ë˜ëŠ” ì „ì²´ í¬ë¡¤ë§ ê³¼ì •ì—ì„œ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {start_err}")


if __name__ == "__main__":
    #store_first_db()
    asyncio.run(crawler())