import asyncio
import json
import random
import time
from asyncio import wait_for
from multiprocessing.managers import PoolProxy
from re import search
from time import sleep
import lxml
import platform
from bs4 import BeautifulSoup
import chardet
import pandas as pd
import pprint
import sqlite3
import zendriver as zd
import urllib
import re
from re import search, sub, compile as re_compile # compile ì¶”ê°€
import os
import sys

headless = False

def store_first_db():

    file_path = 'fulldata_07_24_04_P_ì¼ë°˜ìŒì‹ì .csv'

    with open(file_path, 'rb') as f:
        sample = f.read(1024 * 1024)  # 1MB

    # ìƒ˜í”Œ ë°ì´í„°ë¡œ ì¸ì½”ë”© ê°ì§€
    encoding_data = chardet.detect(sample)
    detected_encoding = encoding_data['encoding']
    print(f"Detected encoding: {detected_encoding}")

    data = pd.read_csv(file_path, encoding=detected_encoding, low_memory=False)
    pprint.pprint(data.head())
    print("Data loaded successfully.")

    # for column in data.columns:
    #     unique_values = data[column].dropna().unique()
    #     print(f"ğŸ“Œ ì»¬ëŸ¼: {column}")
    #     print(f"   â–¶ ê³ ìœ ê°’ {len(unique_values)}ê°œ")
    #     print(f"   â–¶ ìƒ˜í”Œ: {unique_values[:10]}")  # ì²˜ìŒ 10ê°œë§Œ ë³´ì—¬ì¤Œ
    #     print("-" * 50)

    #check data length
    print(f"Total records: {len(data)}")

    # íì—… ì œì™¸
    if 'ì˜ì—…ìƒíƒœëª…' in data.columns:
        print("ğŸ› ï¸ 'ì˜ì—…ìƒíƒœëª…' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ íì—… ë°ì´í„° ê±¸ëŸ¬ë‚´ëŠ” ì¤‘...")
        data = data[data['ì˜ì—…ìƒíƒœëª…'] != 'íì—…']
    else:
        print("âš ï¸ 'ì˜ì—…ìƒíƒœëª…' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. íì—… ë°ì´í„° ê±¸ëŸ¬ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    conn = sqlite3.connect('food_data.db')

    # DataFrameì„ SQLite í…Œì´ë¸”ë¡œ ì €ì¥
    data.to_sql('restaurants', conn, if_exists='replace', index=False)

    # ì—°ê²° ëŠê¸°
    conn.close()

    print(f"âœ… íì—… ì œì™¸ í›„ {len(data)}ê±´ ì €ì¥ ì™„ë£Œ! (DB: food_data.db, Table: restaurants)")

def load_10_restaurant_names_and_addresses():
    conn = sqlite3.connect('food_data.db')
    cursor = conn.cursor()
    cursor.execute("SELECT ë²ˆí˜¸, ì‚¬ì—…ì¥ëª…, ë„ë¡œëª…ì „ì²´ì£¼ì†Œ FROM restaurants where CRAWL = 0 LIMIT 200;")
    rows = cursor.fetchall()

    conn.close()

    restaurant_infos = []
    for row in rows:
        id, business_name, road_address = row
        if id and business_name and road_address:
            restaurant_infos.append((id, business_name, road_address))
    return restaurant_infos

def load_restaurant_subset(start, end):
    conn = sqlite3.connect("/app/food_data.db")
    cursor = conn.cursor()
    cursor.execute(
        "SELECT ë²ˆí˜¸, ì‚¬ì—…ì¥ëª…, ë„ë¡œëª…ì „ì²´ì£¼ì†Œ FROM restaurants LIMIT ? OFFSET ?",
        (end - start, start),
    )
    rows = cursor.fetchall()
    conn.close()
    return [(id, name, addr) for id, name, addr in rows if name and addr]

def make_search_query(business_name, road_address):
    # ë„ë¡œëª… ì£¼ì†Œ ì• 3ë‹¨ê³„ê¹Œì§€ë§Œ
    parts = road_address.split()
    if len(parts) >= 3:
        filtered_address = ' '.join(parts[:3])
    else:
        filtered_address = road_address

    query = f"{business_name} {filtered_address}"
    # pprint.pprint(f"Search query: {query}")
    return query

def sanitize_filename(name):
    name = re.sub(r'[\\/*?:"<>|]', "", name).replace(" ", "_")
    return name[:100] if len(name) > 100 else name

async def load_page_with_wait(browser, url):
    page = await browser.get(url)

    tmp_parser = BeautifulSoup(await page.get_content(), "lxml")
    if any(err in tmp_parser.get_text().lower() for err in [
        "500", "internal server error", "proxy error", "nginx", "html error", "bad gateway"
    ]):
        time.sleep(3)
        raise Exception("âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: ì„œë²„ ì˜¤ë¥˜")
    await page.wait_for("div.place_business_list_wrapper", timeout=10)
    return page


async def load_page_with_wait02(browser, url):
    page = await browser.get(url)
    tmp_parser = BeautifulSoup(await page.get_content(), "lxml")
    if any(err in tmp_parser.get_text().lower() for err in [
        "500", "internal server error", "proxy error", "nginx", "html error", "bad gateway"
    ]):
        time.sleep(3)
        raise Exception("âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: ì„œë²„ ì˜¤ë¥˜")
    await page.wait_for("div.place_fixed_maintab", timeout=10)
    return page


async def with_retry(func, retries=30, delay=1):
    for attempt in range(retries):
        try:
            return await func()
        except Exception as e:
            print(f"âš ï¸ ì¬ì‹œë„ {attempt+1}/{retries} ì‹¤íŒ¨: {e}")
            await asyncio.sleep(delay)
    raise Exception("âŒ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")


async def wait_for_selector_with_retry(page, selector, timeout=10, interval=1):
    max_attempts = int(timeout / interval)
    for attempt in range(max_attempts):
        try:
            node = await page.select_all(selector)
            if node:
                return node
        except Exception:
            pass
        await asyncio.sleep(interval)
    raise TimeoutError(f"âŒ '{selector}' ë¡œë”© ì‹¤íŒ¨ (timeout={timeout}s)")


def extract_dynamic_place_info(soup: BeautifulSoup):
    place_info = {}
    title_block = soup.select("div.zD5Nm > div#_title")
    info_blocks = soup.select("div.PIbes > div.O8qbU")
    if title_block:
        title = title_block[0].get_text(strip=True)
        place_info["title"] = title

    for block in info_blocks:
        key_elem = block.select_one("strong > span.place_blind")
        value_block = block.select_one("div.vV_z_")
        if not key_elem or not value_block:
            continue
        key = key_elem.get_text(strip=True)
        if key == "ì£¼ì†Œ":
            addr = value_block.select_one("span.LDgIH")
            place_info["ì£¼ì†Œ"] = addr.text.strip() if addr else None
        elif key == "ì „í™”ë²ˆí˜¸":
            phone = value_block.select_one("span.xlx7Q")
            place_info["ì „í™”ë²ˆí˜¸"] = phone.text.strip() if phone else None
        elif key == "ì˜ì—…ì‹œê°„":
            status = value_block.select_one("em")
            hours = value_block.select_one("time")
            place_info["ì˜ì—…ìƒíƒœ"] = status.text.strip() if status else None
            place_info["ì˜ì—…ì‹œê°„"] = hours.text.strip() if hours else None
        elif key == "í™ˆí˜ì´ì§€":
            links = value_block.select("a.place_bluelink")
            place_info["í™ˆí˜ì´ì§€ë“¤"] = [a["href"] for a in links if a.get("href")]
        else:
            place_info[key] = value_block.get_text(strip=True)
    return place_info

def append_to_json_file(data, filepath):
    # íŒŒì¼ì´ ìˆìœ¼ë©´ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            try:
                existing = json.load(f)
            except json.JSONDecodeError:
                existing = []
    else:
        existing = []

    # ì¤‘ë³µ ë°©ì§€ (title ê¸°ì¤€)
    titles = {entry.get("title") for entry in existing}
    if data.get("title") not in titles:
        existing.append(data)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(existing, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“¦ ì €ì¥ ì™„ë£Œ: {data['title']}")
    else:
        print(f"âš ï¸ ì¤‘ë³µìœ¼ë¡œ ì €ì¥ ê±´ë„ˆëœ€: {data['title']}")


def log_error_json(error_info, filepath):
    error_info["timestamp"] = time.strftime("%Y-%m-%d %H:%M:%S")
    with open(filepath, "a", encoding="utf-8") as f:
        f.write(json.dumps(error_info, ensure_ascii=False) + "\n")
    print(f"âŒ ì˜¤ë¥˜ ê¸°ë¡ ì™„ë£Œ: {error_info['title'] if 'title' in error_info else 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}")


async def with_browser_retry(browser_ref, executable, browser_args, coro_fn, retries=30, delay=2):
    for attempt in range(retries):
        try:
            result = await coro_fn(browser_ref[0])
            html = await result.get_content()
            soup = BeautifulSoup(html, "lxml")
            page_text = soup.get_text().lower()
            for err in [
                "500", "internal server error", "proxy error", "nginx",
                "sigkill", "sigtrap", "aw snap", "í˜ì´ì§€ë¥¼ í‘œì‹œí•˜ëŠ” ë„ì¤‘ ë¬¸ì œ"
            ]:
                if err in page_text:
                    print(f"âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: ì—ëŸ¬ íƒì§€ë¨ â†’ '{err}'")
                    raise Exception(f"ğŸ›‘ HTML ë‚´ ì—ëŸ¬ í˜ì´ì§€ íƒì§€ë¨: '{err}'")
            return result
        except Exception as e:
            print(f"âš ï¸ ë¸Œë¼ìš°ì € ì‘ì—… ì‹¤íŒ¨ {attempt+1}/{retries}: {e}")
            try:
                await browser_ref[0].stop()
            except:
                pass
            print("ğŸ”„ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì¤‘...")
            browser_ref = [await start_browser(executable)]
            await asyncio.sleep(delay)
    raise Exception("âŒ ë¸Œë¼ìš°ì € ì¬ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨")


async def start_browser(executable):
    return await zd.start(
        headless=headless,
        browser_executable_path=executable,
        browser_args=browser_args
    )

async def wait_for_browser_ready(port, timeout=10):
    import socket
    for _ in range(timeout):
        try:
            with socket.create_connection(("127.0.0.1", port), timeout=1):
                return True
        except:
            await asyncio.sleep(1)
    raise Exception(f"ğŸš« ë¸Œë¼ìš°ì € í¬íŠ¸ {port} ì—°ê²° ì‹¤íŒ¨")


async def with_browser_get(url, browser_ref, executable, retries=30, delay=2):
    for attempt in range(retries):
        try:
            print(f"ğŸ“¡ ì‹œë„ {attempt+1}/{retries}: {url}")
            page = await browser_ref[0].get(url)
            html = await page.get_content()
            soup = BeautifulSoup(html, "lxml")
            if any(err in soup.get_text().lower() for err in [
                "500", "internal server error", "proxy error", "nginx", "html error", "bad gateway"
            ]):
                raise Exception("ğŸ›‘ HTML ë‚´ ì—ëŸ¬ í˜ì´ì§€ íƒì§€ë¨")
            return page
        except Exception as e:
            print(f"âš ï¸ ë¸Œë¼ìš°ì € ì‘ì—… ì‹¤íŒ¨ {attempt+1}/{retries}: {e}")
            print("ğŸ”„ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì¤‘...")
            try:
                await browser_ref[0].stop()
            except:
                pass
            await asyncio.sleep(delay)
            browser_ref[0] = await start_browser(executable)
    raise Exception("âŒ ë¸Œë¼ìš°ì € ì¬ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨")

def normalize(text: str) -> str:
    if not text:
        return ''
    text = re.sub(r'\s+', '', text)                 # ëª¨ë“  ê³µë°± ì œê±°
    text = re.sub(r'[^\wê°€-í£]', '', text)           # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'[\u200b\u200c\u200d\ufeff\xa0]', '', text)  # ë¹„ê°€ì‹œ ë¬¸ì ì œê±°
    return text.lower()

def normalize_address_for_comparison(addr: str) -> str:
    """Specific normalization for address comparison."""
    if not addr:
        return ''
    # Remove floor info, parenthesized details, and all whitespace
    addr = re.sub(r'(?:ì§€í•˜|ì§€ìƒ)?\s?\d+ì¸µ', '', addr.strip()).strip()
    addr = re.sub(r'\b\d+í˜¸\b', '', addr.strip()).strip() # Remove building unit number like 201í˜¸
    addr = re.sub(r'\(.*?\)', '', addr.strip()).strip()
    addr = re.sub(r'\s+', '', addr)
    # Keep essential address characters (Hangul, numbers, basic separators if needed later, but remove for now)
    # Keep commas and hyphens which might be part of address numbers (e.g., 232-9)
    addr = re.sub(r'[^\wê°€-í£,-]', '', addr)
    return addr.lower()

def extract_space_items(html_text: str):
    """
    PC í˜ì´ì§€ HTMLì—ì„œ class="space_title" ìš”ì†Œë¥¼ ëª¨ë‘ ì¶”ì¶œí•˜ì—¬
    ì¥ì†Œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    parser = BeautifulSoup(html_text, "lxml")
    title_tags = parser.select("strong.space_title")

    place_names = []
    for tag in title_tags:
        name = tag.get_text(strip=True)
        if name:
            place_names.append(name)

    if not place_names:
        print("ğŸŸ¡ ì¥ì†Œ ì´ë¦„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"âœ… ì¥ì†Œ ì´ë¦„ {len(place_names)}ê°œ ì¶”ì¶œ ì™„ë£Œ.")
    return place_names

def extract_apollo_place_items(html_text: str):
    """
    Extracts place summary items from the __APOLLO_STATE__ JSON in the HTML.
    """
    parser = BeautifulSoup(html_text, "lxml")
    scripts = parser.find_all("script")

    apollo_data_raw = None
    for script in scripts:
        # Check if the script content exists and contains the target string
        if script.string and "window.__APOLLO_STATE__" in script.string:
            # --- Corrected Regex ---
            # Removed the '$' anchor to match even if other JS code follows the APOLLO_STATE assignment.
            # Added non-greedy match {.*?} just in case of nested structures, though {.*} often works too.
            match = re.search(r"window\.__APOLLO_STATE__\s*=\s*({.*?});", script.string, re.DOTALL)
            if match:
                apollo_data_raw = match.group(1)
                print("âœ… APOLLO_STATE ìŠ¤í¬ë¦½íŠ¸ ë¸”ë¡ ì°¾ìŒ.")
                break # Exit the loop once found

    if not apollo_data_raw:
        print("ğŸŸ¡ APOLLO_STATE ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return []

    try:
        # Attempt to load the extracted string as JSON
        apollo_json = json.loads(apollo_data_raw)
        print("âœ… APOLLO_STATE JSON íŒŒì‹± ì„±ê³µ.")
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        # Optional: Print a snippet of the raw data for debugging JSON errors
        # print("--- íŒŒì‹± ì‹œë„í•œ ë°ì´í„° (ì¼ë¶€) ---")
        # print(apollo_data_raw[:500] + "..." if apollo_data_raw else "N/A")
        # print("-----------------------------")
        return []

    place_items = []
    # Iterate through the parsed JSON dictionary
    for key, value in apollo_json.items():
        # Check if the key is a string, starts with "PlaceSummary:", and the value is a dictionary
        if isinstance(key, str) and key.startswith("PlaceSummary:") and isinstance(value, dict):
             # Optionally add more specific checks, e.g., if value must contain '__typename'
             # if '__typename' in value and value['__typename'] == 'PlaceSummary':
            place_items.append(value)

    print(f"âœ… APOLLO_STATE ë‚´ PlaceSummary í•­ëª© {len(place_items)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
    return place_items



def extract_rq_items(html_text: str):
    """
    window.__RQ_STREAMING_STATE__.push({...}); ë¸”ë¡ ì•ˆì˜ JSONì„ ì¶”ì¶œí•´ì„œ
    items ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ë¶€ ë°˜í™˜í•œë‹¤.
    """
    parser = BeautifulSoup(html_text, "lxml")
    scripts = parser.find_all("script")
    # pprint.pprint(scripts) # ë””ë²„ê¹…ìš©

    # window.__RQ_STREAMING_STATE__.push(...) ë¥¼ ì°¾ëŠ” ì •ê·œì‹
    # JSON ê°ì²´ê°€ ë³µì¡í•˜ê³  ì—¬ëŸ¬ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê´€ëŒ€í•œ íŒ¨í„´ ì‚¬ìš©
    push_regex = re_compile(
        r'window\.__RQ_STREAMING_STATE__\.push\((.*?)\)\s*;?\s*$', # ëë¶€ë¶„ ê³µë°±/ì„¸ë¯¸ì½œë¡  í—ˆìš©
        re.DOTALL | re.MULTILINE # ì—¬ëŸ¬ ì¤„ì— ê±¸ì³ ë§¤ì¹­
    )

    all_items = []
    found_pushes = 0

    for script in scripts:
        if not script.string:
            continue

        # ì •ê·œì‹ìœ¼ë¡œ push í˜¸ì¶œ ë¶€ë¶„ ì°¾ê¸°
        matches = push_regex.findall(script.string.strip()) # ìŠ¤í¬ë¦½íŠ¸ ë‚´ìš© ì•ë’¤ ê³µë°± ì œê±°
        # pprint.pprint(matches) # ë””ë²„ê¹…ìš©
        for match in matches:
            found_pushes += 1
            # print(f"DEBUG: Found push content: {match[:200]}...") # ë””ë²„ê¹…ìš©
            try:
                # JSON íŒŒì‹± ì‹œë„
                parsed = json.loads(match)

                # 'queries' í‚¤ í™•ì¸ ë° ìˆœíšŒ
                queries = parsed.get("queries", [])
                if not isinstance(queries, list):
                    # print("DEBUG: 'queries' is not a list.")
                    continue

                for q_index, q in enumerate(queries):
                    # 'state', 'data', 'items' ê²½ë¡œ í™•ì¸
                    items = q.get("state", {}).get("data", {}).get("items", [])

                    # itemsê°€ ë¦¬ìŠ¤íŠ¸ì´ê³  ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
                    if isinstance(items, list) and items:
                        print(f"âœ… Script ë‚´ì—ì„œ {len(items)}ê°œì˜ items ë°œê²¬ (query index: {q_index})")
                        all_items.extend(items) # ì°¾ì€ ì•„ì´í…œ ì¶”ê°€

            except json.JSONDecodeError as e:
                # print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}. Content: {match[:300]}...") # ë””ë²„ê¹…ìš©
                continue # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ë§¤ì¹˜ ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë¡œ
            except Exception as e:
                print(f"âš ï¸ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                continue

    if found_pushes == 0:
         print("ğŸŸ¡ RQ_STREAMING_STATE push í˜¸ì¶œì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    elif not all_items:
        print("ğŸŸ¡ push í˜¸ì¶œì€ ì°¾ì•˜ìœ¼ë‚˜, ìœ íš¨í•œ 'items' ë°ì´í„°ë¥¼ í¬í•¨í•œ í˜¸ì¶œì´ ì—†ì—ˆìŠµë‹ˆë‹¤.")
    else:
         print(f"âœ… ìµœì¢… ì¶”ì¶œëœ items: {len(all_items)}ê°œ")

    return all_items


async def crawler():
    restaurant_infos = load_10_restaurant_names_and_addresses()

    if not restaurant_infos:
        print("âŒ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ê²Œ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    print(f"â„¹ï¸ {len(restaurant_infos)}ê°œ ê°€ê²Œì— ëŒ€í•œ í¬ë¡¤ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    system = platform.platform()
    arch = platform.machine()
    executable = None

    print("ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸")
    print(f"ì‹œìŠ¤í…œ: {system}")
    print(f"ì•„í‚¤í…ì²˜: {arch}")

    if system != "mac" and arch in ("aarch64", "arm64"):
        print("ARM64 í™˜ê²½ ê°ì§€")
        if os.path.exists("/usr/bin/ungoogled-chromium"):
            executable = "/usr/bin/ungoogled-chromium"
        elif os.path.exists("/usr/bin/chromium"):
            executable = "/usr/bin/chromium"

    try:
        browser_ref = [await start_browser(executable)]

        print("âœ… Zendriver ì‹œì‘ ì™„ë£Œ.")
        success, fail, need_check = 0, 0, 0

        for index, (id, business_name, road_address) in enumerate(restaurant_infos):
            if (index + 1) % 10 == 0:
                await browser_ref[0].stop()
                print("ğŸ”„ ë©”ëª¨ë¦¬ ìœ ì¶œ ë°©ì§€ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì¤‘...")
                browser_ref = [await start_browser(executable)]
                print("âœ… ë©”ëª¨ë¦¬ ìœ ì¶œ ë°©ì§€ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì™„ë£Œ.")

            try:
                #search_query = make_search_query(business_name, road_address)
                search_query = road_address
                encoded_query = urllib.parse.quote(search_query)
                mob_url = f"https://m.place.naver.com/place/searchByAddress/addressPlace?query={encoded_query}&x=126&y=37"
                print(f"ğŸ”— [{index + 1} | {len(restaurant_infos)}] {business_name}")
                print(f"ğŸ”— [{index + 1} | {len(restaurant_infos)}] {search_query} URL: {mob_url}")

                page = await with_browser_get(mob_url, browser_ref, executable, retries=30, delay=3)
                await page.wait_for("header", timeout=10)
                html_src = await page.get_content()
                # Extract potential matches from Apollo state
                items = extract_apollo_place_items(html_src)

                if not items:
                    print(f"âš ï¸ [{index + 1} | {len(restaurant_infos)}] Apollo items ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ì—†ìŒ: {business_name}")
                    need_check += 1
                    log_error_json({"id": id, "title": business_name, "address": road_address, "url": mob_url, "error": "No Apollo items found"}, os.path.join(ERROR_DIR, f"error_log_{start_index}.json"))
                    continue

                # --- Matching Logic ---
                normalized_db_name = normalize(business_name)
                normalized_db_addr = normalize_address_for_comparison(road_address)
                #print(f"   ì •ê·œí™”ëœ DB ì´ë¦„: '{normalized_db_name}'")
                #print(f"   ì •ê·œí™”ëœ DB ì£¼ì†Œ: '{normalized_db_addr}'")

                name_matches = []
                for item in items:
                    item_name = item.get("name", "")
                    normalized_item_name = normalize(item_name)
                    #print(f"   - ê²€ì‚¬ì¤‘ì¸ ì´ë¦„: '{item_name}' (ì •ê·œí™”: '{normalized_item_name}')") # Debug print
                    if normalized_item_name == normalized_db_name:
                        name_matches.append(item)
                        print(f"     âœ”ï¸ ì´ë¦„ ì¼ì¹˜!")

                if not name_matches:
                    print(f"ğŸŸ¡ [{index + 1} | {len(restaurant_infos)}] ì´ë¦„ ì¼ì¹˜ í•­ëª© ì—†ìŒ: '{business_name}'")
                    need_check += 1
                    log_error_json({"id": id, "title": business_name, "address": road_address, "url": mob_url, "error": "No name match in Apollo items", "found_names": [i.get('name') for i in items]}, os.path.join(ERROR_DIR, f"error_log_{start_index}.json"))
                    continue

                elif len(name_matches) == 1:
                    best_match = name_matches[0]
                    print(f"âœ… [{index + 1} | {len(restaurant_infos)}] ì´ë¦„ ìœ ì¼ ë§¤ì¹­ ì„±ê³µ: '{best_match.get('name')}' (ID: {best_match.get('id')})")

                else: # Multiple name matches, use address to disambiguate
                    print(f"âš ï¸ [{index + 1} | {len(restaurant_infos)}] '{business_name}' ì´ë¦„ ì¼ì¹˜ í•­ëª© {len(name_matches)}ê°œ ë°œê²¬. ì£¼ì†Œ ë¹„êµ ì‹œë„...")
                    found_address_match = False
                    for match in name_matches:
                        addr_to_check = match.get("roadAddress") or match.get("address") # Prefer road address
                        normalized_item_addr = normalize_address_for_comparison(addr_to_check)

                        print(f"   - ë¹„êµ ëŒ€ìƒ ì£¼ì†Œ: '{addr_to_check}' (ì •ê·œí™”: '{normalized_item_addr}')")

                        # Check if normalized DB address is contained within the normalized item address
                        # This is slightly more flexible than exact match
                        if normalized_db_addr in normalized_item_addr:
                            best_match = match
                            print(f"âœ… [{index + 1} | {len(restaurant_infos)}] ì£¼ì†Œ í¬í•¨ í™•ì¸: '{best_match.get('name')}' (ID: {best_match.get('id')})")
                            found_address_match = True
                            break # Use the first address match found

                    if not found_address_match:
                        best_match = name_matches[0] # Fallback to the first name match if no address matches
                        print(f"ğŸŸ¡ [{index + 1} | {len(restaurant_infos)}] ì£¼ì†Œ ì¼ì¹˜/í¬í•¨ ì—†ìŒ. ì²« ë²ˆì§¸ ì´ë¦„ ì¼ì¹˜ í•­ëª© ì‚¬ìš©: '{best_match.get('name')}' (ID: {best_match.get('id')})")
                # --- End Matching Logic ---

                if best_match and best_match.get("id"):
                    best = best_match
                    business_name = best.get("name")
                    print(f"âœ… [{index + 1} | {len(restaurant_infos)}] ìµœì¢… ë§¤ì¹­ ì„±ê³µ: '{business_name}' (ID: {best['id']})")

                await with_browser_retry(
                    browser_ref, executable, browser_args,
                    lambda b: b.get(f"https://m.place.naver.com/place/{best['id']}")
                )
                print(f"ğŸ”— [{index + 1} | {len(restaurant_infos)}] {f"https://m.place.naver.com/place/{best['id']}"} ë¡œë”© ì™„ë£Œ")
                print(f"ğŸ”—[{index + 1} | {len(restaurant_infos)}] {search_query} 2ì°¨ URL: https://m.place.naver.com/place/{best['id']}")
                await with_retry(lambda: page.wait_for("div.place_fixed_maintab", timeout=10))

                parser = BeautifulSoup(await page.get_content(), "lxml")
                main_tab = parser.select_one('div[class="place_fixed_maintab"]')
                if main_tab:
                    href_list = [
                        a['href']
                        for a in main_tab.select('a[href]')
                        if a['href'].strip() and not a['href'].strip().startswith('#')
                    ]
                    print(f"ğŸ½ï¸ [{index + 1} | {len(restaurant_infos)}] {search_query} ìœ íš¨í•œ ë§í¬ ê°œìˆ˜: {len(href_list)}")
                    print(f"ğŸ½ï¸ [{index + 1} | {len(restaurant_infos)}] {search_query} ë§í¬: {href_list}")
                else:
                    print(f"âŒ [{index + 1} | {len(restaurant_infos)}] {search_query} place_fixed_maintab not found.")

                place_info = extract_dynamic_place_info(parser)

                data = {
                    "id": id,
                    "query": search_query,
                    "title": business_name,
                    "place_info": place_info,
                    "unique_links": f'/place/{best['id']}',
                    "tab_list": href_list,
                    "url": mob_url
                }
                print(data)

                append_to_json_file(data, output_path)
                success += 1

            except Exception as e:
                print(f"âŒ [{index + 1} | {len(restaurant_infos)}] JSON ë§¤ì¹­ ì‹¤íŒ¨: {e}")
                fail += 1
                continue


    finally:
        await browser_ref[0].stop()
        print("ğŸ›‘ Zendriver ì¢…ë£Œ ì™„ë£Œ")
        print("ğŸ›‘ í¬ë¡¤ëŸ¬ ì¢…ë£Œ ì™„ë£Œ")
        print(f"\nâœ… ì™„ë£Œ: {success} / âŒ ì‹¤íŒ¨: {fail} / âš ï¸ í™•ì¸ í•„ìš”: {need_check}")


if __name__ == "__main__":
    #store_first_db()
    if not os.path.exists("screenshots"):
        os.makedirs("screenshots")

    if not os.path.exists("web_data"):
        os.makedirs("web_data")

    if not os.path.exists("error_logs"):
        os.makedirs("error_logs")

    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    SCREENSHOT_DIR = os.path.join(BASE_DIR, "screenshots")
    DATA_DIR = os.path.join(BASE_DIR, "web_data")
    ERROR_DIR = os.path.join(BASE_DIR, "error_logs")

    os.makedirs(SCREENSHOT_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(ERROR_DIR, exist_ok=True)

    print("ğŸ“‚ Screenshot ì €ì¥ ê²½ë¡œ:", SCREENSHOT_DIR)
    print("ğŸ“‚ WebData ì €ì¥ ê²½ë¡œ:", DATA_DIR)
    print("ğŸ“‚ ErrorLog ì €ì¥ ê²½ë¡œ:", ERROR_DIR)

    start_index = int(os.environ.get("crawl_second_START_INDEX", sys.argv[1] if len(sys.argv) > 1 else 0))
    output_path = os.path.join(DATA_DIR, f"crawl_second_output_{start_index}.json")

    browser_args = [
        "--no-sandbox",
        "--disable-dev-shm-usage",
        "--disable-setuid-sandbox",
        "--disable-software-rasterizer",
        "--disable-blink-features=AutomationControlled",
        "--window-size=1280x800",  # ë°˜ë“œì‹œ ì‚¬ì´ì¦ˆ ì§€ì •
        "--start-maximized",  # headlessì—ì„œë„ ìµœëŒ€í™”ì²˜ëŸ¼ ë³´ì´ê²Œ
        "--disable-infobars",
        "--disable-extensions",
        "--disable-popup-blocking",
        "--enable-logging=stderr",
        "--log-level=1",
    ]

    asyncio.run(crawler())